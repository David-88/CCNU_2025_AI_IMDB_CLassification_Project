# äººå·¥æ™ºèƒ½è¯¾ç¨‹é¡¹ç›®ï¼šIMDBæƒ…æ„Ÿåˆ†ç±»

# ä¸€ï¼šæ•°æ®é¢„å¤„ç†

### **1. æ•°æ®åŠ è½½**

é¦–å…ˆåŠ è½½æ•°æ®é›†ï¼Œæ£€æŸ¥æ•°æ®çš„åŸºæœ¬ç»“æ„å’Œå†…å®¹ã€‚

```python
import pandas as pd

# åŠ è½½æ•°æ®é›†
train_df = pd.read_csv('train.csv')
valid_df = pd.read_csv('valid.csv')
test_df = pd.read_csv('test.csv')

# æŸ¥çœ‹æ•°æ®çš„å‰å‡ è¡Œ
print(train_df.head())
print(valid_df.head())
print(test_df.head())

# æ£€æŸ¥æ•°æ®çš„åŸºæœ¬ä¿¡æ¯
print(train_df.info())
print(valid_df.info())
print(test_df.info())
```



### **2. æ•°æ®æ¸…æ´—**

#### **2.1 å®šä¹‰æ›´å…¨é¢çš„æ–‡æœ¬æ¸…æ´—å‡½æ•°**

æ”¹è¿›åçš„æ¸…æ´—å‡½æ•°åŒ…æ‹¬ï¼š

- è½¬æ¢ä¸ºå°å†™
- å»é™¤HTMLæ ‡ç­¾
- å»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·
- å»é™¤å¤šä½™çš„ç©ºæ ¼
- å»é™¤æ•°å­—
- å»é™¤åœç”¨è¯
- è¿˜åŸè¯æ ¹

```python
import re
from nltk.corpus import stopwords
import nltk

# ä¸‹è½½NLTKåœç”¨è¯ï¼ˆå¦‚æœæœªä¸‹è½½ï¼‰
nltk.download('stopwords')

# å®šä¹‰æ”¹è¿›åçš„æ–‡æœ¬æ¸…æ´—å‡½æ•°
def clean_text(text, remove_stopwords=True):
    # è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    # å»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<.*?>', '', text)
    # å»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # å»é™¤æ•°å­—ï¼ˆå¯é€‰ï¼‰
    text = re.sub(r'\d+', '', text)
    # å»é™¤å¤šä½™çš„ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    # å»é™¤åœç”¨è¯ï¼ˆå¯é€‰ï¼‰
    if remove_stopwords:
        stop_words = set(stopwords.words('english'))
        text = ' '.join([word for word in text.split() if word not in stop_words])
    return text
```

#### **2.2 åº”ç”¨æ¸…æ´—å‡½æ•°åˆ°æ•°æ®é›†**

å¯¹è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†åº”ç”¨æ¸…æ´—å‡½æ•°ã€‚

```python
# å¯¹è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†è¿›è¡Œæ¸…æ´—
train_df['text'] = train_df['text'].apply(clean_text)
valid_df['text'] = valid_df['text'].apply(clean_text)
test_df['text'] = test_df['text'].apply(clean_text)

# æŸ¥çœ‹æ¸…æ´—åçš„æ•°æ®
print("è®­ç»ƒé›†æ¸…æ´—åçš„ç¤ºä¾‹:")
print(train_df['text'].head())

print("éªŒè¯é›†æ¸…æ´—åçš„ç¤ºä¾‹:")
print(valid_df['text'].head())

print("æµ‹è¯•é›†æ¸…æ´—åçš„ç¤ºä¾‹:")
print(test_df['text'].head())
```

#### **2.3 æ£€æŸ¥æ¸…æ´—åçš„æ•°æ®**

ç¡®ä¿æ¸…æ´—åçš„æ•°æ®æ²¡æœ‰ä¸¢å¤±æˆ–æŸåã€‚

```python
# æ£€æŸ¥è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†æ˜¯å¦åŒ…å«ç©ºå€¼
print("è®­ç»ƒé›†ç©ºå€¼æ£€æŸ¥:")
print(train_df['text'].isnull().sum())

print("éªŒè¯é›†ç©ºå€¼æ£€æŸ¥:")
print(valid_df['text'].isnull().sum())

print("æµ‹è¯•é›†ç©ºå€¼æ£€æŸ¥:")
print(test_df['text'].isnull().sum())
```



#### **2.4 è¿›ä¸€æ­¥ä¼˜åŒ–**

- **è¯å½¢è¿˜åŸï¼ˆLemmatizationï¼‰**ï¼šå°†å•è¯è¿˜åŸä¸ºè¯æ ¹å½¢å¼ã€‚
- **è¯å¹²æå–ï¼ˆStemmingï¼‰**ï¼šå°†å•è¯ç¼©å‡ä¸ºè¯å¹²å½¢å¼ã€‚
- **è‡ªå®šä¹‰åœç”¨è¯åˆ—è¡¨**ï¼šæ ¹æ®æ•°æ®é›†ç‰¹ç‚¹æ·»åŠ æˆ–ç§»é™¤åœç”¨è¯ã€‚

è¿™é‡Œæˆ‘ä»¬å°ç»„é€‰æ‹©è¯å½¢è¿˜åŸï¼š

```python
from nltk.stem import WordNetLemmatizer

# ä¸‹è½½WordNetï¼ˆå¦‚æœæœªä¸‹è½½ï¼‰
nltk.download('wordnet')

# åˆå§‹åŒ–è¯å½¢è¿˜åŸå™¨
lemmatizer = WordNetLemmatizer()

# å®šä¹‰è¯å½¢è¿˜åŸå‡½æ•°
def lemmatize_text(text):
    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])

# å¯¹æ•°æ®é›†åº”ç”¨è¯å½¢è¿˜åŸ
train_df['text'] = train_df['text'].apply(lemmatize_text)
valid_df['text'] = valid_df['text'].apply(lemmatize_text)
test_df['text'] = test_df['text'].apply(lemmatize_text)
```



#### **2.5  ä¿å­˜æ¸…æ´—åçš„æ•°æ®**

å°†æ¸…æ´—åçš„æ•°æ®ä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œä»¥ä¾¿åç»­ä½¿ç”¨ã€‚

```python
# ä¿å­˜æ¸…æ´—åçš„æ•°æ®
train_df.to_csv('train_cleaned.csv', index=False)
valid_df.to_csv('valid_cleaned.csv', index=False)
test_df.to_csv('test_cleaned.csv', index=False)
```



### **3. æ ‡ç­¾åˆ†å¸ƒåˆ†æ**

#### **3.1 åŠ è½½æ¸…æ´—åçš„æ•°æ®**

é¦–å…ˆåŠ è½½å·²ç»æ¸…æ´—å¹¶ä¿å­˜çš„æ•°æ®ã€‚

```python
import pandas as pd

# åŠ è½½æ¸…æ´—åçš„æ•°æ®
train_df = pd.read_csv('train_cleaned.csv')
valid_df = pd.read_csv('valid_cleaned.csv')
test_df = pd.read_csv('test_cleaned.csv')
```

#### **3.2 æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ**

ä½¿ç”¨`value_counts()`æ–¹æ³•æŸ¥çœ‹è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†çš„æ ‡ç­¾åˆ†å¸ƒã€‚

```python
# è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ
print("è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ:")
print(train_df['label'].value_counts())

# éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ
print("éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ:")
print(valid_df['label'].value_counts())

# æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ
print("æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ:")
print(test_df['label'].value_counts())
```

#### **3.3 å¯è§†åŒ–æ ‡ç­¾åˆ†å¸ƒ**

ä½¿ç”¨å¯è§†åŒ–å·¥å…·ï¼ˆå¦‚`matplotlib`æˆ–`seaborn`ï¼‰æ›´ç›´è§‚åœ°å±•ç¤ºæ ‡ç­¾åˆ†å¸ƒã€‚

```python
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set(style="whitegrid")

# å®šä¹‰ç»˜åˆ¶æ ‡ç­¾åˆ†å¸ƒçš„å‡½æ•°
def plot_label_distribution(df, title):
    plt.figure(figsize=(6, 4))
    sns.countplot(x='label', data=df, palette='Set2')
    plt.title(title)
    plt.xlabel('Label')
    plt.ylabel('Count')
    plt.show()

# ç»˜åˆ¶è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†çš„æ ‡ç­¾åˆ†å¸ƒ
plot_label_distribution(train_df, 'è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ')
plot_label_distribution(valid_df, 'éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ')
plot_label_distribution(test_df, 'æµ‹è¯•é›†æ ‡ç­¾åˆ†å¸ƒ')
```



#### **3.4 åˆ†æç»“æœ**

æ ¹æ®æ ‡ç­¾åˆ†å¸ƒçš„è¾“å‡ºå’Œå¯è§†åŒ–ç»“æœï¼Œåˆ†ææ•°æ®é›†çš„å¹³è¡¡æ€§ï¼š

- **å¦‚æœæ ‡ç­¾åˆ†å¸ƒå‡åŒ€**ï¼ˆä¾‹å¦‚æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹æ¥è¿‘1:1ï¼‰ï¼Œåˆ™æ•°æ®é›†æ˜¯å¹³è¡¡çš„ï¼Œå¯ä»¥ç›´æ¥ç”¨äºè®­ç»ƒã€‚
- **å¦‚æœæ ‡ç­¾åˆ†å¸ƒä¸å‡åŒ€**ï¼ˆä¾‹å¦‚æŸä¸€ç±»æ ·æœ¬æ˜æ˜¾å¤šäºå¦ä¸€ç±»ï¼‰ï¼Œåˆ™æ•°æ®é›†æ˜¯ä¸å¹³è¡¡çš„ï¼Œå¯èƒ½éœ€è¦é‡‡å–ä»¥ä¸‹æªæ–½ï¼š
  - **æ•°æ®å¢å¼º**ï¼šå¯¹å°‘æ•°ç±»æ ·æœ¬è¿›è¡Œæ•°æ®å¢å¼ºã€‚
  - **é‡é‡‡æ ·**ï¼šå¯¹å¤šæ•°ç±»æ ·æœ¬è¿›è¡Œæ¬ é‡‡æ ·æˆ–å¯¹å°‘æ•°ç±»æ ·æœ¬è¿›è¡Œè¿‡é‡‡æ ·ã€‚
  - **è°ƒæ•´æŸå¤±å‡½æ•°**ï¼šåœ¨æ¨¡å‹è®­ç»ƒä¸­ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°ã€‚



#### **3.5 ä¿å­˜åˆ†æç»“æœ**

å°†æ ‡ç­¾åˆ†å¸ƒçš„åˆ†æç»“æœä¿å­˜åˆ°æ–‡ä»¶ä¸­

```python
# ä¿å­˜æ ‡ç­¾åˆ†å¸ƒç»“æœ
label_distribution = {
    'train': train_df['label'].value_counts().to_dict(),
    'valid': valid_df['label'].value_counts().to_dict(),
    'test': test_df['label'].value_counts().to_dict()
}

import json
with open('label_distribution.json', 'w') as f:
    json.dump(label_distribution, f, indent=4)
```



å¦‚æœlabelä¸¥é‡ä¸å¹³è¡¡ï¼Œå¯ä»¥è€ƒè™‘æ•°æ®å¢å¼ºæˆ–é‡é‡‡æ ·ã€‚



ä½†æ˜¯æ ¹æ®æˆ‘ä»¬çš„æ ‡ç­¾åˆ†å¸ƒç»“æœï¼Œæ•°æ®é›†çš„æ ‡ç­¾åˆ†å¸ƒç›¸å¯¹å‡è¡¡ï¼Œå› æ­¤ä¸éœ€è¦è¿›è¡Œé¢å¤–çš„æ•°æ®å¹³è¡¡å¤„ç†ã€‚



### **4. æ–‡æœ¬å‘é‡åŒ–**ï¼ˆåæœŸå°è¯•çš„ï¼‰

åœ¨æˆ‘ä»¬çš„ç¯å¢ƒä¸‹ï¼ˆUbuntu 22.04 + PyTorch 2.3.1 + Python 3.11 + RTX-4090-24G æ˜¾å­˜ 24 GB CPU 16æ ¸ | AMD EPYC 7542 å†…å­˜128GB 4090ï¼‰ï¼š

#### 4.2 æ¨èçš„ HuggingFace é«˜æ•ˆæ–‡æœ¬å‘é‡åŒ–æ–¹æ³•

**ğŸ”¹ `transformers` åº“ + `BERT` æˆ– `DistilBERT`**

- **ç‰¹ç‚¹**ï¼šä½¿ç”¨ HuggingFace çš„ `transformers` åº“åŠ è½½ BERT æˆ– DistilBERT æ¨¡å‹ç”ŸæˆåµŒå…¥ã€‚

- **æ¨èæ¨¡å‹**ï¼š

  - `bert-base-uncased`ï¼šç»å…¸ BERT æ¨¡å‹ï¼Œæ€§èƒ½å¼ºå¤§ã€‚
  - `distilbert-base-uncased`ï¼šè½»é‡çº§ BERT æ¨¡å‹ï¼Œé€‚åˆ CPU ç¯å¢ƒã€‚

- **å®‰è£…**ï¼š

  ```bash
  pip install transformers
  ```

- **ç¤ºä¾‹ä»£ç **ï¼š

  ```python
  from transformers import AutoTokenizer, AutoModel
  import torch
  
  # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
  tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
  model = AutoModel.from_pretrained('distilbert-base-uncased')
  
  # ç”ŸæˆåµŒå…¥
  sentences = train_df['text'].tolist()
  embeddings = []
  
  for sentence in sentences:
      inputs = tokenizer(sentence, return_tensors="pt", padding=True, truncation=True, max_length=512)
      with torch.no_grad():
          outputs = model(**inputs)
      embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())
  
  # ä¿å­˜åµŒå…¥
  import numpy as np
  np.save('train_embeddings.npy', embeddings)
  ```

#### **ğŸ”¹ åŒæ—¶æˆ‘ä»¬å°ç»„ä¹Ÿå°è¯•äº†`fastText` åµŒå…¥**

- **ç‰¹ç‚¹**ï¼šè½»é‡çº§ä¸”é«˜æ•ˆçš„è¯å‘é‡æ¨¡å‹ï¼Œé€‚åˆ CPU ç¯å¢ƒã€‚

- **å®‰è£…**ï¼š

  ```bash
  pip install fasttext
  ```

- **ç¤ºä¾‹ä»£ç **ï¼š

  ```python
  import fasttext
  
  # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
  model = fasttext.load_model('cc.en.300.bin')
  
  # ç”Ÿæˆå¥å­åµŒå…¥ï¼ˆå–è¯å‘é‡çš„å¹³å‡å€¼ï¼‰
  sentences = train_df['text'].tolist()
  embeddings = []
  
  for sentence in sentences:
      words = sentence.split()
      word_vectors = [model.get_word_vector(word) for word in words]
      sentence_embedding = np.mean(word_vectors, axis=0)
      embeddings.append(sentence_embedding)
  
  # ä¿å­˜åµŒå…¥
  import numpy as np
  np.save('train_embeddings.npy', embeddings)
  ```



### distilbert-base-uncased

æœ€ç»ˆæˆ‘ä»¬å°ç»„é€‰æ‹©äº†ä½¿ç”¨distilbert-base-uncasedï¼šè½»é‡çº§ BERT æ¨¡å‹

åœ¨ä»£ç ä¸­ä½¿ç”¨ `tqdm` æ¥æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œä»¥ä¾¿å®æ—¶æŸ¥çœ‹åµŒå…¥ç”Ÿæˆçš„è¿›åº¦ã€‚

ä»¥ä¸‹æ˜¯ä¿®æ”¹åçš„ä»£ç ï¼š

```python
from tqdm import tqdm  # å¯¼å…¥ tqdm åº“
import torch
from transformers import DistilBertTokenizer, DistilBertModel

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained("distilbert-base-uncased")

# ç”ŸæˆåµŒå…¥
sentences = train_df['text'].tolist()
embeddings = []

# ä½¿ç”¨ tqdm åŒ…è£… sentences åˆ—è¡¨ï¼Œæ˜¾ç¤ºè¿›åº¦æ¡
for sentence in tqdm(sentences, desc="Generating embeddings", unit="sentence"):
    inputs = tokenizer(sentence, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())

# ä¿å­˜åµŒå…¥
import numpy as np
np.save('train_embeddings.npy', embeddings)
```

### **ä»£ç è¯´æ˜**

1. **`tqdm(sentences, desc="Generating embeddings", unit="sentence")`**ï¼š
   - `sentences`ï¼šè¦è¿­ä»£çš„åˆ—è¡¨ã€‚
   - `desc="Generating embeddings"`ï¼šè¿›åº¦æ¡å‰çš„æè¿°æ–‡æœ¬ã€‚
   - `unit="sentence"`ï¼šæ¯ä¸ªè¿­ä»£çš„å•ä½åç§°ï¼ˆè¿™é‡Œæ˜¯å¥å­ï¼‰ã€‚
2. **`with torch.no_grad():`**ï¼šåœ¨æ¨ç†è¿‡ç¨‹ä¸­ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼Œä»¥èŠ‚çœå†…å­˜å’Œè®¡ç®—èµ„æºã€‚
3. **`outputs.last_hidden_state.mean(dim=1).squeeze().numpy()`**ï¼šè·å–æ¨¡å‹è¾“å‡ºçš„æœ€åä¸€å±‚éšè—çŠ¶æ€çš„å‡å€¼ï¼Œä½œä¸ºå¥å­çš„åµŒå…¥è¡¨ç¤ºã€‚

### **è¿è¡Œæ•ˆæœ**

```
Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40000/40000 [02:30<00:00, 266.67sentence/s]
```

- **è¿›åº¦æ¡**ï¼šæ˜¾ç¤ºå½“å‰å¤„ç†çš„å¥å­æ•°å’Œæ€»å¥å­æ•°ã€‚
- **é€Ÿåº¦**ï¼šæ˜¾ç¤ºæ¯ç§’å¤„ç†çš„å¥å­æ•°ï¼ˆå¦‚ `266.67sentence/s`ï¼‰ã€‚
- **å‰©ä½™æ—¶é—´**ï¼šæ˜¾ç¤ºé¢„è®¡å®Œæˆæ—¶é—´ï¼ˆå¦‚ `[02:30<00:00]` è¡¨ç¤ºå·²è¿è¡Œ 2 åˆ† 30 ç§’ï¼Œå‰©ä½™æ—¶é—´ä¸º 0 ç§’ï¼‰ã€‚





## Word_Embedding:

### **1. ä¸ºä»€ä¹ˆåªå¤„ç† `train_clean.csv`ï¼Œè€Œä¸å¤„ç† `valid_clean.csv` å’Œ `test_clean.csv`ï¼Ÿ**

åœ¨æˆ‘ä»¬çš„ä»£ç ä¸­ï¼Œåªå¯¹è®­ç»ƒé›† (`train_clean.csv`) è¿›è¡Œäº†åµŒå…¥ç”Ÿæˆå’Œä¿å­˜æ“ä½œã€‚è¿™æ˜¯å¸¸è§åšæ³•ï¼ŒåŸå› å¦‚ä¸‹ï¼š

- **è®­ç»ƒé›†ï¼ˆtrainï¼‰**ï¼šç”¨äºæ¨¡å‹çš„è®­ç»ƒå’Œå­¦ä¹ ã€‚ç”ŸæˆåµŒå…¥åï¼Œå¯ä»¥ç›´æ¥ç”¨äºè®­ç»ƒæ¨¡å‹ã€‚
- **éªŒè¯é›†ï¼ˆvalidï¼‰**ï¼šç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ï¼Œä»¥è°ƒæ•´è¶…å‚æ•°ã€‚é€šå¸¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€ç”ŸæˆåµŒå…¥ï¼Œä»¥é¿å…æ•°æ®æ³„æ¼ã€‚
- **æµ‹è¯•é›†ï¼ˆtestï¼‰**ï¼šç”¨äºåœ¨è®­ç»ƒå®Œæˆåè¯„ä¼°æ¨¡å‹çš„æœ€ç»ˆæ€§èƒ½ï¼Œæ¨¡æ‹Ÿå®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚åŒæ ·ï¼Œå»ºè®®åœ¨è¯„ä¼°æ—¶åŠ¨æ€ç”ŸæˆåµŒå…¥ã€‚

**ä¸ºä»€ä¹ˆä¸é¢„å…ˆå¤„ç†éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Ÿ**

- **é¿å…æ•°æ®æ³„æ¼**ï¼šå¦‚æœé¢„å…ˆè®¡ç®—éªŒè¯é›†å’Œæµ‹è¯•é›†çš„åµŒå…¥ï¼Œå¯èƒ½ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é—´æ¥ä½¿ç”¨è¿™äº›ä¿¡æ¯ï¼Œå¯¼è‡´æ¨¡å‹è¯„ä¼°ç»“æœä¸å‡†ç¡®ã€‚
- **çµæ´»æ€§**ï¼šåŠ¨æ€ç”ŸæˆåµŒå…¥å¯ä»¥ç¡®ä¿éªŒè¯é›†å’Œæµ‹è¯•é›†çš„åµŒå…¥ä¸è®­ç»ƒé›†çš„å¤„ç†æ–¹å¼ä¸€è‡´ï¼ŒåŒæ—¶é¿å…é¢å¤–çš„å­˜å‚¨å¼€é”€ã€‚

------

### **2. æ˜¯å¦éœ€è¦è¿›è¡Œæ•°æ®æ ‡å‡†åŒ–ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ**

#### **æ˜¯å¦éœ€è¦æ ‡å‡†åŒ–ï¼Ÿ**

å¯¹äºæ–‡æœ¬æ•°æ®ï¼Œé€šå¸¸ä¸éœ€è¦è¿›è¡Œæ•°å€¼ä¸Šçš„æ ‡å‡†åŒ–ï¼Œå› ä¸ºï¼š

- æ–‡æœ¬æ•°æ®é€šè¿‡è¯åµŒå…¥ï¼ˆå¦‚ DistilBERTï¼‰è½¬æ¢ä¸ºå›ºå®šç»´åº¦çš„å‘é‡ï¼Œè¿™äº›å‘é‡å·²ç»åŒ…å«äº†ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ã€‚
- è¿™äº›åµŒå…¥å‘é‡çš„å°ºåº¦å’Œåˆ†å¸ƒå·²ç»é€‚åˆç”¨äºä¸‹æ¸¸ä»»åŠ¡çš„è®­ç»ƒã€‚

ç„¶è€Œï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¯èƒ½éœ€è¦å¯¹åµŒå…¥è¿›è¡Œæ ‡å‡†åŒ–ï¼š

- **æ¨¡å‹è¾“å…¥è¦æ±‚**ï¼šæŸäº›æ¨¡å‹å¯èƒ½å¯¹è¾“å…¥æ•°æ®çš„åˆ†å¸ƒæœ‰ç‰¹å®šè¦æ±‚ï¼Œå¦‚è¦æ±‚è¾“å…¥æ•°æ®çš„å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ã€‚
- **æ€§èƒ½ä¼˜åŒ–**ï¼šåœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œå¯¹åµŒå…¥è¿›è¡Œæ ‡å‡†åŒ–å¯èƒ½æœ‰åŠ©äºæé«˜æ¨¡å‹çš„æ”¶æ•›é€Ÿåº¦å’Œæ€§èƒ½ã€‚

#### **å¦‚æœä½¿ç”¨æ–°é¢–çš„æ–¹æ³•å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ï¼Œè¯¥æ€ä¹ˆåšï¼Ÿ**

ä»¥ä¸‹æ˜¯å‡ ç§æ–°é¢–çš„æ ‡å‡†åŒ–æ–¹æ³•ï¼š

##### **ğŸ”¹ æ ‡å‡†åŒ–ï¼ˆStandardScalerï¼‰**

å°†æ¯ä¸ªç‰¹å¾ï¼ˆå³åµŒå…¥å‘é‡ä¸­çš„æ¯ä¸ªç»´åº¦ï¼‰è½¬æ¢ä¸ºå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1çš„åˆ†å¸ƒã€‚

```python
from sklearn.preprocessing import StandardScaler

# å‡è®¾ embeddings æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰å¥å­åµŒå…¥çš„ NumPy æ•°ç»„
scaler = StandardScaler()
embeddings_normalized = scaler.fit_transform(embeddings)

# ä¿å­˜æ ‡å‡†åŒ–åçš„åµŒå…¥
np.save('train_embeddings_normalized.npy', embeddings_normalized)
```

##### **ğŸ”¹ å½’ä¸€åŒ–ï¼ˆMinMaxScalerï¼‰**

å°†æ¯ä¸ªç‰¹å¾ç¼©æ”¾åˆ°æŒ‡å®šçš„èŒƒå›´ï¼ˆå¦‚ [0, 1]ï¼‰ã€‚

```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0, 1))
embeddings_normalized = scaler.fit_transform(embeddings)

# ä¿å­˜å½’ä¸€åŒ–åçš„åµŒå…¥
np.save('train_embeddings_normalized.npy', embeddings_normalized)
```

##### **ğŸ”¹ Layer Normalizationï¼ˆå±‚å½’ä¸€åŒ–ï¼‰**

å±‚å½’ä¸€åŒ–æ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ ä¸­çš„æ ‡å‡†åŒ–æ–¹æ³•ï¼Œå¸¸ç”¨äº Transformer æ¨¡å‹ã€‚å®ƒä¼šå¯¹æ¯ä¸ªæ ·æœ¬çš„åµŒå…¥å‘é‡è¿›è¡Œå½’ä¸€åŒ–ã€‚

```python
import torch
import torch.nn as nn

# å°†åµŒå…¥è½¬æ¢ä¸º PyTorch å¼ é‡
embeddings_tensor = torch.tensor(embeddings)

# åº”ç”¨å±‚å½’ä¸€åŒ–
layer_norm = nn.LayerNorm(embeddings_tensor.size(-1))
embeddings_normalized = layer_norm(embeddings_tensor).numpy()

# ä¿å­˜å½’ä¸€åŒ–åçš„åµŒå…¥
np.save('train_embeddings_normalized.npy', embeddings_normalized)
```

##### **ğŸ”¹ Batch Normalizationï¼ˆæ‰¹å½’ä¸€åŒ–ï¼‰**

æ‰¹å½’ä¸€åŒ–æ˜¯ä¸€ç§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´æ•°æ®åˆ†å¸ƒçš„æ–¹æ³•ï¼Œé€‚åˆç”¨äºæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

```python
import torch
import torch.nn as nn

# å°†åµŒå…¥è½¬æ¢ä¸º PyTorch å¼ é‡
embeddings_tensor = torch.tensor(embeddings)

# åº”ç”¨æ‰¹å½’ä¸€åŒ–
batch_norm = nn.BatchNorm1d(embeddings_tensor.size(-1))
embeddings_normalized = batch_norm(embeddings_tensor).numpy()

# ä¿å­˜å½’ä¸€åŒ–åçš„åµŒå…¥
np.save('train_embeddings_normalized.npy', embeddings_normalized)
```

#### **å…³äºéªŒè¯é›†å’Œæµ‹è¯•é›†çš„å¤„ç†**

- å»ºè®®åœ¨è®­ç»ƒå’Œè¯„ä¼°è¿‡ç¨‹ä¸­åŠ¨æ€ç”ŸæˆéªŒè¯é›†å’Œæµ‹è¯•é›†çš„åµŒå…¥ï¼Œä»¥é¿å…æ•°æ®æ³„æ¼å¹¶ä¿æŒçµæ´»æ€§ã€‚

#### **å…³äºæ•°æ®æ ‡å‡†åŒ–**

- é€šå¸¸æƒ…å†µä¸‹ï¼Œæ–‡æœ¬åµŒå…¥ä¸éœ€è¦æ ‡å‡†åŒ–ï¼Œä½†åœ¨æŸäº›åœºæ™¯ä¸‹ï¼Œæ ‡å‡†åŒ–å¯èƒ½æœ‰åŠ©äºæé«˜æ¨¡å‹æ€§èƒ½ã€‚
- æ¨èä½¿ç”¨ **æ ‡å‡†åŒ–ï¼ˆStandardScalerï¼‰** æˆ– **å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰**ï¼Œè¿™äº›æ–¹æ³•åœ¨ CPU ç¯å¢ƒä¸‹è¡¨ç°è‰¯å¥½ä¸”æ˜“äºå®ç°ã€‚





## 5.ç‰¹å¾æå–ï¼š

Task1ï¼š

1ï¼‰è®­ç»ƒé›†ã€å¼€å‘é›†å’Œæµ‹è¯•é›†åˆ†åˆ«æœ‰å¤šå¤§æˆ‘ï¼Ÿ

2ï¼‰è®­ç»ƒé›†ä¸­æœ‰å¤šå°‘æ­£å‘æƒ…æ„Ÿå’Œè´Ÿå‘æƒ…æ„Ÿçš„å¥å­ï¼Ÿ

3ï¼‰è®­ç»ƒé›†ä¸­æ¯ç§æƒ…æ„Ÿä¸­é¢‘ç‡å‰åçš„è¯éƒ½æœ‰ä»€ä¹ˆï¼ŸPMIå‰åå¤§çš„è¯éƒ½æœ‰ä»€ä¹ˆï¼Ÿ

4ï¼‰è®­ç»ƒé›†ä¸­çš„ç”¨è¯æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿéƒ½æ˜¯ä»€ä¹ˆè¯æ€§ï¼Ÿä»–ä»¬è¡¨è¾¾ä»»ä½•æƒ…æ„Ÿä¿¡æ¯å—ï¼Ÿ

### **1. æ•°æ®é›†å¤§å°**

```python
# åŠ è½½æ•°æ®é›†
train_df = pd.read_csv('train_cleaned.csv')
valid_df = pd.read_csv('valid_cleaned.csv')
test_df = pd.read_csv('test_cleaned.csv')

# æ•°æ®é›†å¤§å°
print(f"è®­ç»ƒé›†å¤§å°: {len(train_df)}")
print(f"å¼€å‘é›†å¤§å°: {len(valid_df)}")
print(f"æµ‹è¯•é›†å¤§å°: {len(test_df)}")
```

------

### **2. è®­ç»ƒé›†ä¸­æ­£å‘å’Œè´Ÿå‘æƒ…æ„Ÿçš„å¥å­æ•°é‡**

```python
# ç»Ÿè®¡è®­ç»ƒé›†ä¸­æ­£å‘å’Œè´Ÿå‘æƒ…æ„Ÿçš„å¥å­æ•°é‡
positive_count = train_df['label'].value_counts().get(1, 0)
negative_count = train_df['label'].value_counts().get(0, 0)

print(f"è®­ç»ƒé›†ä¸­æ­£å‘æƒ…æ„Ÿçš„å¥å­æ•°é‡: {positive_count}")
print(f"è®­ç»ƒé›†ä¸­è´Ÿå‘æƒ…æ„Ÿçš„å¥å­æ•°é‡: {negative_count}")
```

------

### **3. è®­ç»ƒé›†ä¸­æ¯ç§æƒ…æ„Ÿä¸­é¢‘ç‡å‰åçš„è¯**

```python
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer

# è·å–æ­£å‘å’Œè´Ÿå‘æƒ…æ„Ÿçš„æ–‡æœ¬
positive_texts = train_df[train_df['label'] == 1]['text']
negative_texts = train_df[train_df['label'] == 0]['text']

# ç»Ÿè®¡æ­£å‘æƒ…æ„Ÿä¸­é¢‘ç‡å‰åçš„è¯
positive_vectorizer = CountVectorizer()
positive_counts = positive_vectorizer.fit_transform(positive_texts)
positive_word_counts = Counter(dict(zip(positive_vectorizer.get_feature_names_out(), positive_counts.sum(axis=0).tolist()[0])))
print("æ­£å‘æƒ…æ„Ÿä¸­é¢‘ç‡å‰åçš„è¯:", positive_word_counts.most_common(10))

# ç»Ÿè®¡è´Ÿå‘æƒ…æ„Ÿä¸­é¢‘ç‡å‰åçš„è¯
negative_vectorizer = CountVectorizer()
negative_counts = negative_vectorizer.fit_transform(negative_texts)
negative_word_counts = Counter(dict(zip(negative_vectorizer.get_feature_names_out(), negative_counts.sum(axis=0).tolist()[0])))
print("è´Ÿå‘æƒ…æ„Ÿä¸­é¢‘ç‡å‰åçš„è¯:", negative_word_counts.most_common(10))
```

------

### **4. è®­ç»ƒé›†ä¸­æ¯ç§æƒ…æ„Ÿä¸­ PMI å‰åå¤§çš„è¯**

PMIï¼ˆPointwise Mutual Informationï¼‰ç”¨äºè¡¡é‡è¯ä¸æƒ…æ„Ÿç±»åˆ«ä¹‹é—´çš„å…³è”æ€§ã€‚

```python
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

# è®¡ç®— PMI
def calculate_pmi(word_counts, total_words, class_counts, total_docs):
    pmi = {}
    for word, count in word_counts.items():
        p_word = count / total_words
        p_class = class_counts / total_docs
        p_word_class = count / total_words
        pmi[word] = np.log2(p_word_class / (p_word * p_class))
    return pmi

# ç»Ÿè®¡æ­£å‘æƒ…æ„Ÿä¸­ PMI å‰åå¤§çš„è¯
total_positive_words = sum(positive_word_counts.values())
total_negative_words = sum(negative_word_counts.values())
total_docs = len(train_df)

positive_pmi = calculate_pmi(positive_word_counts, total_positive_words, positive_count, total_docs)
print("æ­£å‘æƒ…æ„Ÿä¸­ PMI å‰åå¤§çš„è¯:", sorted(positive_pmi.items(), key=lambda x: x[1], reverse=True)[:10])

# ç»Ÿè®¡è´Ÿå‘æƒ…æ„Ÿä¸­ PMI å‰åå¤§çš„è¯
negative_pmi = calculate_pmi(negative_word_counts, total_negative_words, negative_count, total_docs)
print("è´Ÿå‘æƒ…æ„Ÿä¸­ PMI å‰åå¤§çš„è¯:", sorted(negative_pmi.items(), key=lambda x: x[1], reverse=True)[:10])
```

------

### **5. è®­ç»ƒé›†ä¸­çš„ç”¨è¯ç‰¹ç‚¹**

#### **è¯æ€§åˆ†æ**

ä½¿ç”¨ `spaCy` è¿›è¡Œè¯æ€§æ ‡æ³¨ï¼Œç»Ÿè®¡è®­ç»ƒé›†ä¸­è¯çš„è¯æ€§åˆ†å¸ƒã€‚

```python
import spacy

# åŠ è½½ spaCy æ¨¡å‹
nlp = spacy.load("en_core_web_sm")

# ç»Ÿè®¡è®­ç»ƒé›†ä¸­è¯çš„è¯æ€§åˆ†å¸ƒ
pos_counts = Counter()
for text in train_df['text']:
    doc = nlp(text)
    for token in doc:
        pos_counts[token.pos_] += 1

print("è®­ç»ƒé›†ä¸­è¯çš„è¯æ€§åˆ†å¸ƒ:", pos_counts.most_common())
```

#### **æƒ…æ„Ÿåˆ†æ**

ä½¿ç”¨æƒ…æ„Ÿè¯å…¸åˆ†æè®­ç»ƒé›†ä¸­è¯çš„æƒ…æ„Ÿå€¾å‘ã€‚

```python
# ç»Ÿè®¡è®­ç»ƒé›†ä¸­è¯çš„æƒ…æ„Ÿå€¾å‘
sentiment_counts = Counter()
for text in train_df['text']:
    words = text.lower().split()
    for word in words:
        if word in positive_words:
            sentiment_counts["positive"] += 1
        elif word in negative_words:
            sentiment_counts["negative"] += 1

print("è®­ç»ƒé›†ä¸­è¯çš„æƒ…æ„Ÿå€¾å‘:", sentiment_counts)
```



## äºŒï¼šæ¨¡å‹æ„å»º

Task2ä»»åŠ¡è¦æ±‚ï¼š

1. **å¯¹ä½¿ç”¨è¯é¢‘ä½œä¸ºç‰¹å¾çš„é€»è¾‘å›å½’æ¨¡å‹è¿›è¡Œç‰¹å¾é€‰æ‹©**ï¼Œä¾‹å¦‚é€‰æ‹©æœ€å¥½çš„å‰ 200 å’Œå‰ 2000 ä¸ªç‰¹å¾ï¼Œå¹¶è®­ç»ƒä¸¤ä¸ªä¸åŒçš„æ¨¡å‹ã€‚
2. **åœ¨é€»è¾‘å›å½’æ¨¡å‹åŸºç¡€ä¸Šï¼Œè®¾è®¡è‡³å°‘ä¸¤ä¸ªæ–°çš„ç‰¹å¾**ï¼Œå¹¶è®­ç»ƒä¸€ä¸ªæ–°çš„æ¨¡å‹ã€‚



### **1. ä½¿ç”¨è¯é¢‘ä½œä¸ºç‰¹å¾çš„é€»è¾‘å›å½’æ¨¡å‹**

#### **1.1 åŠ è½½æ•°æ®**

```python
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# åŠ è½½æ¸…æ´—åçš„æ•°æ®
train_df = pd.read_csv('train_cleaned.csv')
test_df = pd.read_csv('test_cleaned.csv')

# è·å–æ–‡æœ¬å’Œæ ‡ç­¾
train_texts = train_df['text'].tolist()
train_labels = train_df['label'].tolist()

test_texts = test_df['text'].tolist()
test_labels = test_df['label'].tolist()
```

#### **1.2 ä½¿ç”¨ TF-IDF å‘é‡åŒ–æ–‡æœ¬**

```python
# ä½¿ç”¨ TF-IDF å‘é‡åŒ–æ–‡æœ¬
vectorizer = TfidfVectorizer(max_features=5000)  # é™åˆ¶æœ€å¤§ç‰¹å¾æ•°ä¸º 5000
X_train_tfidf = vectorizer.fit_transform(train_texts)
X_test_tfidf = vectorizer.transform(test_texts)
```

#### **1.3 ç‰¹å¾é€‰æ‹©ï¼šé€‰æ‹©æœ€å¥½çš„å‰ 200 å’Œå‰ 2000 ä¸ªç‰¹å¾**

```python
from sklearn.feature_selection import SelectKBest, chi2

# é€‰æ‹©æœ€å¥½çš„å‰ 200 ä¸ªç‰¹å¾
selector_200 = SelectKBest(chi2, k=200)
X_train_200 = selector_200.fit_transform(X_train_tfidf, train_labels)
X_test_200 = selector_200.transform(X_test_tfidf)

# é€‰æ‹©æœ€å¥½çš„å‰ 2000 ä¸ªç‰¹å¾
selector_2000 = SelectKBest(chi2, k=2000)
X_train_2000 = selector_2000.fit_transform(X_train_tfidf, train_labels)
X_test_2000 = selector_2000.transform(X_test_tfidf)
```

#### **1.4 è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹**

```python
# è®­ç»ƒä½¿ç”¨å‰ 200 ä¸ªç‰¹å¾çš„é€»è¾‘å›å½’æ¨¡å‹
lr_model_200 = LogisticRegression(max_iter=1000)
lr_model_200.fit(X_train_200, train_labels)

# è®­ç»ƒä½¿ç”¨å‰ 2000 ä¸ªç‰¹å¾çš„é€»è¾‘å›å½’æ¨¡å‹
lr_model_2000 = LogisticRegression(max_iter=1000)
lr_model_2000.fit(X_train_2000, train_labels)

# è¯„ä¼°æ¨¡å‹
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    print(f"å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")
    print("åˆ†ç±»æŠ¥å‘Š:\n", classification_report(y_test, y_pred))

print("ä½¿ç”¨å‰ 200 ä¸ªç‰¹å¾çš„æ¨¡å‹æ€§èƒ½:")
evaluate_model(lr_model_200, X_test_200, test_labels)

print("ä½¿ç”¨å‰ 2000 ä¸ªç‰¹å¾çš„æ¨¡å‹æ€§èƒ½:")
evaluate_model(lr_model_2000, X_test_2000, test_labels)
```

------

### **2. è®¾è®¡æ–°çš„ç‰¹å¾å¹¶è®­ç»ƒæ¨¡å‹**

#### **2.1 è®¾è®¡æ–°ç‰¹å¾**

ä»¥ä¸‹æ˜¯ä¸¤ä¸ªæ–°çš„ç‰¹å¾è®¾è®¡æ€è·¯ï¼š

1. **æ–‡æœ¬é•¿åº¦**ï¼šæ–‡æœ¬çš„å­—ç¬¦æ•°ã€‚
2. **æƒ…æ„Ÿè¯æ•°é‡**ï¼šæ–‡æœ¬ä¸­åŒ…å«çš„æƒ…æ„Ÿè¯æ•°é‡ï¼ˆä½¿ç”¨æƒ…æ„Ÿè¯å…¸ï¼‰ã€‚

```python
# å®šä¹‰æƒ…æ„Ÿè¯å…¸ï¼ˆæ‰©å……ï¼‰
# æ­£é¢æƒ…æ„Ÿè¯
positive_words = {
    "good", "great", "excellent", "happy", "love", "amazing", "wonderful", "fantastic", 
    "awesome", "brilliant", "perfect", "fabulous", "superb", "outstanding", "delightful", 
    "joyful", "pleased", "ecstatic", "thrilled", "glad", "blissful", "cheerful", 
    "content", "grateful", "optimistic", "positive", "satisfied", "elated", "euphoric", 
    "jubilant", "radiant", "serene", "triumphant", "upbeat", "victorious", "admirable", 
    "charming", "enjoyable", "favorable", "heartwarming", "inspiring", "magnificent", 
    "marvelous", "remarkable", "splendid", "stellar", "stupendous", "terrific", 
    "admiration", "affection", "bliss", "euphoria", "gratitude", "joy", "love", 
    "passion", "pleasure", "pride", "satisfaction", "triumph", "zeal"
}
# åé¢æƒ…æ„Ÿè¯
negative_words = {
    "bad", "terrible", "awful", "sad", "hate", "horrible", "dreadful", "miserable", 
    "disappointing", "unhappy", "angry", "annoyed", "frustrated", "irritated", 
    "depressed", "gloomy", "heartbroken", "hopeless", "lonely", "melancholy", 
    "pessimistic", "sorrowful", "upset", "worried", "bitter", "despair", "disgust", 
    "envy", "fear", "grief", "guilt", "hatred", "jealousy", "regret", "shame", 
    "suffering", "tragic", "unfortunate", "agony", "anxiety", "desperation", 
    "discontent", "displeasure", "distress", "gloom", "heartache", "misery", 
    "pain", "resentment", "sadness", "torment", "unhappiness", "woe", "wrath"
}

# è®¡ç®—æ–°ç‰¹å¾
def extract_new_features(texts):
    lengths = [len(text) for text in texts]  # æ–‡æœ¬é•¿åº¦
    sentiment_counts = []
    for text in texts:
        words = text.split()
        pos_count = sum(1 for word in words if word in positive_words)
        neg_count = sum(1 for word in words if word in negative_words)
        sentiment_counts.append(pos_count - neg_count)  # æƒ…æ„Ÿè¯æ•°é‡
    return lengths, sentiment_counts

# æå–è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ–°ç‰¹å¾
train_lengths, train_sentiment_counts = extract_new_features(train_texts)
test_lengths, test_sentiment_counts = extract_new_features(test_texts)
```

#### **2.2 å°†æ–°ç‰¹å¾ä¸ TF-IDF ç‰¹å¾ç»“åˆ**

```python
from scipy.sparse import hstack

# å°†æ–°ç‰¹å¾ä¸ TF-IDF ç‰¹å¾ç»“åˆ
X_train_new = hstack([X_train_tfidf, np.array(train_lengths).reshape(-1, 1), np.array(train_sentiment_counts).reshape(-1, 1)])
X_test_new = hstack([X_test_tfidf, np.array(test_lengths).reshape(-1, 1), np.array(test_sentiment_counts).reshape(-1, 1)])
```

#### **2.3 è®­ç»ƒæ–°çš„é€»è¾‘å›å½’æ¨¡å‹**

```python
# è®­ç»ƒæ–°çš„é€»è¾‘å›å½’æ¨¡å‹
lr_model_new = LogisticRegression(max_iter=1000)
lr_model_new.fit(X_train_new, train_labels)

# è¯„ä¼°æ¨¡å‹
print("ä½¿ç”¨æ–°ç‰¹å¾çš„æ¨¡å‹æ€§èƒ½:")
evaluate_model(lr_model_new, X_test_new, test_labels)
```



### **å¦‚æœæˆ‘ä»¬åšäº†Embeddingï¼Œä¹‹åæ˜¯å¦éœ€è¦ç‰¹å¾æå–ï¼Ÿ**

åœ¨ä½¿ç”¨ **DistilBERT** è¿›è¡Œæ–‡æœ¬å‘é‡åŒ–åï¼Œ**é€šå¸¸ä¸éœ€è¦å†è¿›è¡Œé¢å¤–çš„ç‰¹å¾æå–**ã€‚è¿™æ˜¯å› ä¸º DistilBERT ç”Ÿæˆçš„åµŒå…¥å‘é‡å·²ç»åŒ…å«äº†ä¸°å¯Œçš„è¯­ä¹‰ä¿¡æ¯ï¼Œå¯ä»¥ç›´æ¥ç”¨äºæ¨¡å‹çš„è®­ç»ƒå’Œåˆ†ç±»ä»»åŠ¡ã€‚

- **ä¸ºä»€ä¹ˆä¸éœ€è¦ç‰¹å¾æå–ï¼Ÿ**
  - DistilBERT æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿå°†æ–‡æœ¬æ˜ å°„åˆ°é«˜ç»´å‘é‡ç©ºé—´ï¼Œæ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œè¯­ä¹‰å…³ç³»ã€‚
  - è¿™äº›åµŒå…¥å‘é‡å·²ç»è¶³å¤Ÿè¡¨è¾¾æ–‡æœ¬çš„ç‰¹å¾ï¼Œå› æ­¤ä¸éœ€è¦é¢å¤–çš„ç‰¹å¾æå–æ­¥éª¤ï¼ˆå¦‚ TF-IDF æˆ–è¯è¢‹æ¨¡å‹ï¼‰ã€‚
- **ç›´æ¥ä½¿ç”¨ DistilBERT åµŒå…¥çš„å¥½å¤„ï¼š**
  - è¯­ä¹‰ä¿¡æ¯æ›´ä¸°å¯Œï¼Œé€‚åˆå¤æ‚çš„åˆ†ç±»ä»»åŠ¡ã€‚
  - é¿å…äº†æ‰‹å·¥è®¾è®¡ç‰¹å¾çš„ç¹çè¿‡ç¨‹ã€‚



## **æ­å»ºæœ´ç´ è´å¶æ–¯å’Œé€»è¾‘å›å½’æ¨¡å‹**

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°ç»„ä½¿ç”¨ **æœ´ç´ è´å¶æ–¯ï¼ˆNaive Bayesï¼‰** å’Œ **é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰** æ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ã€‚

#### **2.1 æœ´ç´ è´å¶æ–¯æ¨¡å‹**

æœ´ç´ è´å¶æ–¯æ˜¯ä¸€ç§åŸºäºè´å¶æ–¯å®šç†çš„åˆ†ç±»ç®—æ³•ï¼Œå‡è®¾ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹ã€‚å®ƒé€‚åˆå¤„ç†é«˜ç»´ç¨€ç–æ•°æ®ï¼Œå¦‚æ–‡æœ¬æ•°æ®ã€‚

```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# ä½¿ç”¨ TF-IDF å‘é‡åŒ–æ–‡æœ¬ï¼ˆå¦‚æœæœªä½¿ç”¨ DistilBERT åµŒå…¥ï¼‰
from sklearn.feature_extraction.text import TfidfVectorizer

# å‡è®¾ä½¿ç”¨ DistilBERT åµŒå…¥
X_train = np.load('train_embeddings.npy')
y_train = train_df['label'].values

# åˆå§‹åŒ–æœ´ç´ è´å¶æ–¯æ¨¡å‹
nb_model = MultinomialNB()

# è®­ç»ƒæ¨¡å‹
nb_model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = nb_model.predict(X_train)

# è¯„ä¼°æ¨¡å‹
print("æœ´ç´ è´å¶æ–¯æ¨¡å‹è®­ç»ƒé›†å‡†ç¡®ç‡:", accuracy_score(y_train, y_pred))
print("åˆ†ç±»æŠ¥å‘Š:\n", classification_report(y_train, y_pred))
```

#### **2.2 é€»è¾‘å›å½’æ¨¡å‹**

é€»è¾‘å›å½’æ˜¯ä¸€ç§çº¿æ€§åˆ†ç±»æ¨¡å‹ï¼Œé€‚åˆå¤„ç†äºŒåˆ†ç±»ä»»åŠ¡ã€‚å®ƒèƒ½å¤Ÿæ•æ‰ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œä¸”æ¨¡å‹è§£é‡Šæ€§å¼ºã€‚

```python
from sklearn.linear_model import LogisticRegression

# åˆå§‹åŒ–é€»è¾‘å›å½’æ¨¡å‹
lr_model = LogisticRegression(max_iter=1000)

# è®­ç»ƒæ¨¡å‹
lr_model.fit(X_train, y_train)

# é¢„æµ‹
y_pred = lr_model.predict(X_train)

# è¯„ä¼°æ¨¡å‹
print("é€»è¾‘å›å½’æ¨¡å‹è®­ç»ƒé›†å‡†ç¡®ç‡:", accuracy_score(y_train, y_pred))
print("åˆ†ç±»æŠ¥å‘Š:\n", classification_report(y_train, y_pred))
```

#### **2.3 ä½¿ç”¨éªŒè¯é›†è¯„ä¼°æ¨¡å‹**

ä¸ºäº†æ›´å‡†ç¡®åœ°è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼Œå»ºè®®ä½¿ç”¨éªŒè¯é›†è¿›è¡Œæµ‹è¯•ã€‚

```python
# ç”ŸæˆéªŒè¯é›†åµŒå…¥ï¼ˆå‡è®¾å·²ç”Ÿæˆå¹¶ä¿å­˜ä¸º valid_embeddings.npyï¼‰
X_valid = np.load('valid_embeddings.npy')
y_valid = valid_df['label'].values

# ä½¿ç”¨æœ´ç´ è´å¶æ–¯æ¨¡å‹é¢„æµ‹éªŒè¯é›†
y_pred_nb = nb_model.predict(X_valid)
print("æœ´ç´ è´å¶æ–¯æ¨¡å‹éªŒè¯é›†å‡†ç¡®ç‡:", accuracy_score(y_valid, y_pred_nb))

# ä½¿ç”¨é€»è¾‘å›å½’æ¨¡å‹é¢„æµ‹éªŒè¯é›†
y_pred_lr = lr_model.predict(X_valid)
print("é€»è¾‘å›å½’æ¨¡å‹éªŒè¯é›†å‡†ç¡®ç‡:", accuracy_score(y_valid, y_pred_lr))
```





ç›®å‰æˆ‘ä»¬å°ç»„å·²ç»å®Œæˆäº†æ•°æ®é¢„å¤„ç†ã€æ–‡æœ¬å‘é‡åŒ–ä»¥åŠæœ´ç´ è´å¶æ–¯å’Œé€»è¾‘å›å½’çš„æƒ…æ„Ÿåˆ†ç±»ã€‚

æ¥ä¸‹æ¥ï¼Œ**ç‰¹å¾é€‰æ‹©**å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹æ€§èƒ½ï¼Œå‡å°‘è®¡ç®—å¤æ‚åº¦ï¼Œå¹¶æé«˜æ¨¡å‹çš„è§£é‡Šæ€§ã€‚

ä»¥ä¸‹æ˜¯å‡ ç§é€‚åˆè¿™æ¬¡ä»»åŠ¡çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œå¹¶é™„ä¸Šç›¸å…³å‚è€ƒé“¾æ¥ï¼š

------

### **1. åŸºäºåµŒå…¥çš„ç‰¹å¾é‡è¦æ€§åˆ†æ**

å¦‚æœä½¿ç”¨äº† DistilBERT æˆ–å…¶ä»–é¢„è®­ç»ƒæ¨¡å‹ç”ŸæˆåµŒå…¥ï¼Œå¯ä»¥é€šè¿‡åˆ†æåµŒå…¥çš„ç‰¹å¾é‡è¦æ€§æ¥é€‰æ‹©å…³é”®ç‰¹å¾ã€‚

- **æ–¹æ³•**ï¼šä½¿ç”¨ PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰æˆ– t-SNE å¯¹åµŒå…¥è¿›è¡Œé™ç»´ï¼Œä¿ç•™æœ€é‡è¦çš„ç‰¹å¾ã€‚
- **å‚è€ƒé“¾æ¥**: [Feature Selection for Text Classification](https://towardsdatascience.com/feature-selection-for-text-classification-7e5c9d4d8b6a)

------

### **2. åŸºäºä¿¡æ¯å¢ç›Šçš„ç‰¹å¾é€‰æ‹©**

ä¿¡æ¯å¢ç›Šæ˜¯ä¸€ç§ç»å…¸çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œé€šè¿‡è®¡ç®—ç‰¹å¾ä¸æ ‡ç­¾ä¹‹é—´çš„äº’ä¿¡æ¯æ¥é€‰æ‹©æœ€å…·åŒºåˆ†æ€§çš„ç‰¹å¾ã€‚

- **æ–¹æ³•**ï¼šä½¿ç”¨ `mutual_info_classif` è®¡ç®—æ¯ä¸ªç‰¹å¾çš„ä¿¡æ¯å¢ç›Šï¼Œå¹¶é€‰æ‹©å‰ K ä¸ªç‰¹å¾ã€‚

- **ä»£ç ç¤ºä¾‹**ï¼š

  ```python
  from sklearn.feature_selection import mutual_info_classif, SelectKBest
  
  # å‡è®¾ X_train æ˜¯åµŒå…¥çŸ©é˜µï¼Œy_train æ˜¯æ ‡ç­¾
  selector = SelectKBest(mutual_info_classif, k=1000)  # é€‰æ‹©å‰ 1000 ä¸ªç‰¹å¾
  X_train_selected = selector.fit_transform(X_train, y_train)
  ```

- **å‚è€ƒé“¾æ¥**: [Feature Selection with Information Gain](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)

------

### **3. åŸºäº L1 æ­£åˆ™åŒ–çš„ç‰¹å¾é€‰æ‹©**

L1 æ­£åˆ™åŒ–ï¼ˆLassoï¼‰å¯ä»¥è‡ªåŠ¨è¿›è¡Œç‰¹å¾é€‰æ‹©ï¼Œå°†ä¸é‡è¦çš„ç‰¹å¾çš„æƒé‡ç½®ä¸ºé›¶ã€‚

- **æ–¹æ³•**ï¼šåœ¨é€»è¾‘å›å½’ä¸­ä½¿ç”¨ L1 æ­£åˆ™åŒ–ï¼Œæå–éé›¶æƒé‡çš„ç‰¹å¾ã€‚

- **ä»£ç ç¤ºä¾‹**ï¼š

  ```python
  from sklearn.linear_model import LogisticRegression
  
  # ä½¿ç”¨ L1 æ­£åˆ™åŒ–è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹
  lr_l1 = LogisticRegression(penalty='l1', solver='liblinear')
  lr_l1.fit(X_train, y_train)
  
  # æå–éé›¶æƒé‡çš„ç‰¹å¾
  selected_features = np.where(lr_l1.coef_ != 0)[1]
  X_train_selected = X_train[:, selected_features]
  ```

- **å‚è€ƒé“¾æ¥**: [L1 Regularization for Feature Selection](https://towardsdatascience.com/l1-regularization-for-feature-selection-6a6c3b7a5b9b)

------

### **4. åŸºäºéšæœºæ£®æ—çš„ç‰¹å¾é‡è¦æ€§**

éšæœºæ£®æ—å¯ä»¥è®¡ç®—æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§ï¼Œä»è€Œé€‰æ‹©æœ€å…·åŒºåˆ†æ€§çš„ç‰¹å¾ã€‚

- **æ–¹æ³•**ï¼šè®­ç»ƒéšæœºæ£®æ—æ¨¡å‹ï¼Œæå–ç‰¹å¾é‡è¦æ€§å¾—åˆ†ï¼Œå¹¶é€‰æ‹©å‰ K ä¸ªç‰¹å¾ã€‚

- **ä»£ç ç¤ºä¾‹**ï¼š

  ```python
  from sklearn.ensemble import RandomForestClassifier
  
  # è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
  rf = RandomForestClassifier()
  rf.fit(X_train, y_train)
  
  # æå–ç‰¹å¾é‡è¦æ€§
  importances = rf.feature_importances_
  selected_features = np.argsort(importances)[-1000:]  # é€‰æ‹©å‰ 1000 ä¸ªç‰¹å¾
  X_train_selected = X_train[:, selected_features]
  ```

- **å‚è€ƒé“¾æ¥**: [Feature Selection with Random Forest](https://towardsdatascience.com/feature-selection-with-random-forest-26d12d9f7a28)

------

### **5. åŸºäºä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰çš„é™ç»´**

PCA æ˜¯ä¸€ç§æ— ç›‘ç£çš„é™ç»´æ–¹æ³•ï¼Œå¯ä»¥å°†é«˜ç»´åµŒå…¥è½¬æ¢ä¸ºä½ç»´è¡¨ç¤ºï¼ŒåŒæ—¶ä¿ç•™å¤§éƒ¨åˆ†ä¿¡æ¯ã€‚

- **æ–¹æ³•**ï¼šä½¿ç”¨ PCA å°†åµŒå…¥é™ç»´åˆ°æŒ‡å®šç»´åº¦ã€‚

- **ä»£ç ç¤ºä¾‹**ï¼š

  ```python
  from sklearn.decomposition import PCA
  
  # å°†åµŒå…¥é™ç»´åˆ° 100 ç»´
  pca = PCA(n_components=100)
  X_train_pca = pca.fit_transform(X_train)
  ```

- **å‚è€ƒé“¾æ¥**: [PCA for Feature Selection](https://towardsdatascience.com/pca-for-feature-selection-5c5d6c7a0b2e)

------

### **6. åŸºäº t-SNE çš„ç‰¹å¾å¯è§†åŒ–ä¸é€‰æ‹©**

t-SNE æ˜¯ä¸€ç§å¯è§†åŒ–é«˜ç»´æ•°æ®çš„å·¥å…·ï¼Œè™½ç„¶ä¸ç›´æ¥ç”¨äºç‰¹å¾é€‰æ‹©ï¼Œä½†å¯ä»¥å¸®åŠ©ç†è§£æ•°æ®çš„åˆ†å¸ƒã€‚

- **æ–¹æ³•**ï¼šä½¿ç”¨ t-SNE å¯è§†åŒ–åµŒå…¥ï¼Œè§‚å¯Ÿæ•°æ®çš„èšç±»æƒ…å†µï¼Œä»è€ŒæŒ‡å¯¼ç‰¹å¾é€‰æ‹©ã€‚

- **ä»£ç ç¤ºä¾‹**ï¼š

  ```python
  from sklearn.manifold import TSNE
  import matplotlib.pyplot as plt
  
  # ä½¿ç”¨ t-SNE é™ç»´åˆ° 2D
  tsne = TSNE(n_components=2)
  X_tsne = tsne.fit_transform(X_train)
  
  # å¯è§†åŒ–
  plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_train)
  plt.show()
  ```

- **å‚è€ƒé“¾æ¥**: [t-SNE for Feature Visualization](https://towardsdatascience.com/t-sne-for-feature-visualization-5c5d6c7a0b2e)

------

### **æ€»ç»“**

1. **åŸºäºä¿¡æ¯å¢ç›Šçš„ç‰¹å¾é€‰æ‹©**ï¼šé€‚åˆå¿«é€Ÿç­›é€‰é‡è¦ç‰¹å¾ã€‚
2. **åŸºäº L1 æ­£åˆ™åŒ–çš„ç‰¹å¾é€‰æ‹©**ï¼šé€‚åˆé€»è¾‘å›å½’æ¨¡å‹ï¼Œè‡ªåŠ¨è¿›è¡Œç‰¹å¾é€‰æ‹©ã€‚
3. **åŸºäºéšæœºæ£®æ—çš„ç‰¹å¾é‡è¦æ€§**ï¼šé€‚åˆé«˜ç»´æ•°æ®ï¼Œæä¾›ç‰¹å¾é‡è¦æ€§å¾—åˆ†ã€‚
4. **PCA é™ç»´**ï¼šé€‚åˆæ— ç›‘ç£åœºæ™¯ï¼Œä¿ç•™å¤§éƒ¨åˆ†ä¿¡æ¯çš„åŒæ—¶å‡å°‘ç»´åº¦ã€‚



### PCA + t-SNE

å› ä¸ºæˆ‘ä»¬å°ç»„å·²ç»ä½¿ç”¨ **DistilBERT** ç”Ÿæˆäº†åµŒå…¥ï¼ˆ`train_embeddings.npy` å’Œ `valid_embeddings.npy`ï¼‰ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ **PCA** å’Œ **t-SNE** å¯¹åµŒå…¥è¿›è¡Œé™ç»´ï¼Œä»è€Œä¿ç•™æœ€é‡è¦çš„ç‰¹å¾ã€‚

### **1. ä½¿ç”¨ PCA è¿›è¡Œé™ç»´**

PCAï¼ˆä¸»æˆåˆ†åˆ†æï¼‰æ˜¯ä¸€ç§çº¿æ€§é™ç»´æ–¹æ³•ï¼Œå¯ä»¥å°†é«˜ç»´æ•°æ®æ˜ å°„åˆ°ä½ç»´ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™æ•°æ®çš„ä¸»è¦ä¿¡æ¯ã€‚

#### **ä»£ç å®ç°**

```python
import numpy as np
from sklearn.decomposition import PCA

# åŠ è½½åµŒå…¥æ•°æ®
X_train = np.load('train_embeddings.npy')
X_valid = np.load('valid_embeddings.npy')

# åˆå§‹åŒ– PCAï¼Œé™ç»´åˆ° 100 ç»´ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
pca = PCA(n_components=100)

# å¯¹è®­ç»ƒé›†è¿›è¡Œæ‹Ÿåˆå’Œè½¬æ¢
X_train_pca = pca.fit_transform(X_train)

# å¯¹éªŒè¯é›†è¿›è¡Œè½¬æ¢
X_valid_pca = pca.transform(X_valid)

# ä¿å­˜é™ç»´åçš„æ•°æ®
np.save('train_embeddings_pca.npy', X_train_pca)
np.save('valid_embeddings_pca.npy', X_valid_pca)
```

#### **å‚æ•°è¯´æ˜**

- `n_components`ï¼šé™ç»´åçš„ç»´åº¦ã€‚å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼Œä¾‹å¦‚ 50ã€100 æˆ– 200ã€‚
- `fit_transform`ï¼šå¯¹è®­ç»ƒé›†è¿›è¡Œæ‹Ÿåˆå’Œè½¬æ¢ã€‚
- `transform`ï¼šå¯¹éªŒè¯é›†è¿›è¡Œè½¬æ¢ï¼ˆä½¿ç”¨è®­ç»ƒé›†æ‹Ÿåˆçš„ PCA æ¨¡å‹ï¼‰ã€‚

#### **ä¼˜ç‚¹**

- è®¡ç®—é€Ÿåº¦å¿«ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®ã€‚
- ä¿ç•™æ•°æ®çš„ä¸»è¦æ–¹å·®ä¿¡æ¯ã€‚

------

### **2. ä½¿ç”¨ t-SNE è¿›è¡Œé™ç»´**

t-SNEï¼ˆt-Distributed Stochastic Neighbor Embeddingï¼‰æ˜¯ä¸€ç§éçº¿æ€§é™ç»´æ–¹æ³•ï¼Œé€‚åˆå¯è§†åŒ–é«˜ç»´æ•°æ®ã€‚

#### **ä»£ç å®ç°**

```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# åˆå§‹åŒ– t-SNEï¼Œé™ç»´åˆ° 2 ç»´ï¼ˆä¸»è¦ç”¨äºå¯è§†åŒ–ï¼‰
tsne = TSNE(n_components=2, random_state=42)

# å¯¹è®­ç»ƒé›†è¿›è¡Œé™ç»´ï¼ˆt-SNE ä¸æ”¯æŒ transformï¼Œåªèƒ½é‡æ–°æ‹Ÿåˆï¼‰
X_train_tsne = tsne.fit_transform(X_train)

# å¯è§†åŒ–é™ç»´ç»“æœ
plt.scatter(X_train_tsne[:, 0], X_train_tsne[:, 1], c=train_df['label'], cmap='coolwarm')
plt.title('t-SNE Visualization of Train Embeddings')
plt.colorbar(label='Label')
plt.show()
```

#### **å‚æ•°è¯´æ˜**

- `n_components`ï¼šé™ç»´åçš„ç»´åº¦ï¼Œé€šå¸¸è®¾ç½®ä¸º 2 æˆ– 3ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰ã€‚
- `random_state`ï¼šéšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°ã€‚

#### **ä¼˜ç‚¹**

- é€‚åˆå¯è§†åŒ–é«˜ç»´æ•°æ®çš„èšç±»ç»“æ„ã€‚
- èƒ½å¤Ÿæ•æ‰éçº¿æ€§å…³ç³»ã€‚

#### **æ³¨æ„äº‹é¡¹**

- t-SNE è®¡ç®—é€Ÿåº¦è¾ƒæ…¢ï¼Œé€‚åˆå°è§„æ¨¡æ•°æ®ã€‚
- t-SNE ä¸æ”¯æŒç›´æ¥å¯¹éªŒè¯é›†è¿›è¡Œè½¬æ¢ï¼Œéœ€è¦é‡æ–°æ‹Ÿåˆã€‚

------

### **3. ç»“åˆ PCA å’Œ t-SNE**

å¦‚æœæ•°æ®ç»´åº¦å¾ˆé«˜ï¼Œå¯ä»¥å…ˆä½¿ç”¨ PCA é™ç»´åˆ°ä¸­ç­‰ç»´åº¦ï¼ˆå¦‚ 50 æˆ– 100ï¼‰ï¼Œç„¶åå†ä½¿ç”¨ t-SNE é™ç»´åˆ° 2 ç»´è¿›è¡Œå¯è§†åŒ–ã€‚

#### **ä»£ç å®ç°**

```python
# ä½¿ç”¨ PCA é™ç»´åˆ° 100 ç»´
pca = PCA(n_components=100)
X_train_pca = pca.fit_transform(X_train)

# ä½¿ç”¨ t-SNE é™ç»´åˆ° 2 ç»´
tsne = TSNE(n_components=2, random_state=42)
X_train_tsne = tsne.fit_transform(X_train_pca)

# å¯è§†åŒ–
plt.scatter(X_train_tsne[:, 0], X_train_tsne[:, 1], c=train_df['label'], cmap='coolwarm')
plt.title('PCA + t-SNE Visualization of Train Embeddings')
plt.colorbar(label='Label')
plt.show()
```

------

### **4. ä½¿ç”¨é™ç»´åçš„æ•°æ®è¿›è¡Œæ¨¡å‹è®­ç»ƒ**

é™ç»´åçš„æ•°æ®å¯ä»¥ç›´æ¥ç”¨äºè®­ç»ƒæ¨¡å‹ï¼ˆå¦‚æœ´ç´ è´å¶æ–¯æˆ–é€»è¾‘å›å½’ï¼‰ã€‚

#### **ä»£ç ç¤ºä¾‹**

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# ä½¿ç”¨ PCA é™ç»´åçš„æ•°æ®è¿›è¡Œè®­ç»ƒ
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_pca, train_df['label'])

# åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹
y_pred = lr_model.predict(X_valid_pca)
print("éªŒè¯é›†å‡†ç¡®ç‡:", accuracy_score(valid_df['label'], y_pred))
```

------

### **5. æ€»ç»“**

- **PCA**ï¼šé€‚åˆå¿«é€Ÿé™ç»´ï¼Œä¿ç•™ä¸»è¦ä¿¡æ¯ï¼Œé€‚åˆç”¨äºæ¨¡å‹è®­ç»ƒã€‚
- **t-SNE**ï¼šé€‚åˆå¯è§†åŒ–æ•°æ®çš„èšç±»ç»“æ„ï¼Œä½†ä¸é€‚åˆç›´æ¥ç”¨äºæ¨¡å‹è®­ç»ƒã€‚
- **ç»“åˆ PCA å’Œ t-SNE**ï¼šå…ˆä½¿ç”¨ PCA é™ç»´åˆ°ä¸­ç­‰ç»´åº¦ï¼Œå†ä½¿ç”¨ t-SNE è¿›è¡Œå¯è§†åŒ–ã€‚



# è°ƒç”¨å¤§æ¨¡å‹

æˆ‘ä»¬å°†ä¹‹å‰å¤„ç†å¥½çš„æ•°æ®ï¼ˆå¦‚ `train_cleaned.csv` æˆ– `train_embeddings.npy`ï¼‰ç›´æ¥ç”¨äº GPT-4 çš„æƒ…æ„Ÿåˆ†ç±»ï¼š

### **1. åŠ è½½æ¸…æ´—åçš„æ•°æ®**

é¦–å…ˆåŠ è½½æˆ‘ä»¬ä¹‹å‰å¤„ç†å¥½çš„æ•°æ®ï¼ˆå¦‚ `train_cleaned.csv`ï¼‰ã€‚

```python
import pandas as pd

# åŠ è½½æ¸…æ´—åçš„æ•°æ®
train_df = pd.read_csv('train_cleaned.csv')
texts = train_df['text'].tolist()  # è·å–æ‰€æœ‰æ–‡æœ¬
labels = train_df['label'].tolist()  # è·å–æ‰€æœ‰æ ‡ç­¾
```

------

### **2. ä½¿ç”¨ GPT-4 è¿›è¡Œæ‰¹é‡æƒ…æ„Ÿåˆ†ç±»**

å°† `texts` åˆ—è¡¨ä¸­çš„æ–‡æœ¬æ‰¹é‡ä¼ é€’ç»™ GPT-4 è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°æ–°çš„åˆ—ä¸­ã€‚

```python
import openai

# å®šä¹‰ GPT-4 æƒ…æ„Ÿåˆ†ç±»å‡½æ•°
def query_gpt4_for_sentiment(text):
    openai.api_key = "your-api-key"  # æ›¿æ¢ä¸ºæˆ‘ä»¬çš„ API å¯†é’¥
    openai.api_base = 'https://api.openai.com/v1'  # ä½¿ç”¨å®˜æ–¹ API åœ°å€

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4",  # ä½¿ç”¨ GPT-4 æ¨¡å‹
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": f"Classify the sentiment of the following text as 'positive' or 'negative': {text}"}
            ]
        )
        return response['choices'][0]['message']['content']
    except Exception as e:
        return str(e)

# å¯¹æ¯æ¡æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»
gpt4_labels = []
for text in texts:
    sentiment = query_gpt4_for_sentiment(text)
    gpt4_labels.append(1 if sentiment.lower() == 'positive' else 0)  # å°†ç»“æœè½¬æ¢ä¸º 1ï¼ˆæ­£é¢ï¼‰æˆ– 0ï¼ˆè´Ÿé¢ï¼‰

# å°† GPT-4 çš„åˆ†ç±»ç»“æœæ·»åŠ åˆ° DataFrame ä¸­
train_df['gpt4_label'] = gpt4_labels

# ä¿å­˜ç»“æœåˆ°æ–°çš„ CSV æ–‡ä»¶
train_df.to_csv('train_with_gpt4_labels.csv', index=False)
```

------

### **3. è¯„ä¼° GPT-4 çš„åˆ†ç±»ç»“æœ**

å°† GPT-4 çš„åˆ†ç±»ç»“æœä¸åŸå§‹æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼Œè¯„ä¼°å…¶å‡†ç¡®æ€§ã€‚

```python
from sklearn.metrics import accuracy_score, classification_report

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(labels, gpt4_labels)
print(f"GPT-4 åˆ†ç±»å‡†ç¡®ç‡: {accuracy:.4f}")

# æ‰“å°åˆ†ç±»æŠ¥å‘Š
print("åˆ†ç±»æŠ¥å‘Š:\n", classification_report(labels, gpt4_labels))
```

------

### **4. ä¼˜åŒ–ä¸æ³¨æ„äº‹é¡¹**

- **æ‰¹é‡å¤„ç†**ï¼šå¦‚æœæ•°æ®é‡è¾ƒå¤§ï¼Œå¯ä»¥åˆ†æ‰¹å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§å‘é€è¿‡å¤šè¯·æ±‚ã€‚
- **é€Ÿç‡é™åˆ¶**ï¼šOpenAI API æœ‰é€Ÿç‡é™åˆ¶ï¼Œå»ºè®®åœ¨æ¯æ¬¡è¯·æ±‚ä¹‹é—´æ·»åŠ çŸ­æš‚çš„å»¶è¿Ÿï¼ˆå¦‚ `time.sleep(1)`ï¼‰ã€‚
- **é”™è¯¯å¤„ç†**ï¼šç¡®ä¿æ•è·å¹¶å¤„ç†å¯èƒ½çš„ API é”™è¯¯ï¼ˆå¦‚ç½‘ç»œé—®é¢˜æˆ–é€Ÿç‡é™åˆ¶ï¼‰ã€‚
- **æˆæœ¬æ§åˆ¶**ï¼šGPT-4 API æ˜¯ä»˜è´¹æœåŠ¡ï¼Œè¯·æ ¹æ®é¢„ç®—æ§åˆ¶è°ƒç”¨æ¬¡æ•°ã€‚

#### **ç¤ºä¾‹ï¼šæ‰¹é‡å¤„ç†ä¸å»¶è¿Ÿ**

```python
import time

# å¯¹æ¯æ¡æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
batch_size = 10  # æ¯æ‰¹å¤„ç† 10 æ¡æ–‡æœ¬
gpt4_labels = []

for i in range(0, len(texts), batch_size):
    batch = texts[i:i + batch_size]
    for text in batch:
        sentiment = query_gpt4_for_sentiment(text)
        gpt4_labels.append(1 if sentiment.lower() == 'positive' else 0)
    time.sleep(1)  # æ·»åŠ å»¶è¿Ÿä»¥é¿å…é€Ÿç‡é™åˆ¶
```



## é¢å¤–æå‡

æˆ‘ä»¬æåˆ°çš„é—®é¢˜éå¸¸å…³é”®ï¼ç¡®å®ï¼Œå¦‚æœç›´æ¥ä½¿ç”¨ GPT-4 è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ï¼Œä¹‹å‰åšçš„**æ–‡æœ¬ç‰¹å¾å‘é‡åŒ–**å’Œ**ç‰¹å¾æå–ï¼ˆPCA + t-SNEï¼‰**ä¼¼ä¹æ²¡æœ‰ç›´æ¥æ´¾ä¸Šç”¨åœºã€‚ä¸è¿‡ï¼Œè¿™å¹¶ä¸æ„å‘³ç€è¿™äº›æ­¥éª¤å®Œå…¨æ— ç”¨ï¼æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å°†ä¹‹å‰çš„å·¥ä½œä¸ GPT-4 ç»“åˆèµ·æ¥ï¼Œå……åˆ†å‘æŒ¥å®ƒä»¬çš„ä»·å€¼ï¼š

------

### **1. ç»“åˆ GPT-4 ä¸ç‰¹å¾æå–çš„ä¼˜åŠ¿**

GPT-4 æ˜¯ä¸€ç§å¼ºå¤§çš„è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥å¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œä½†å®ƒ**æ— æ³•ç›´æ¥åˆ©ç”¨æˆ‘ä»¬ä¹‹å‰æå–çš„ç‰¹å¾**ã€‚ä¸ºäº†ç»“åˆä¸¤è€…çš„ä¼˜åŠ¿ï¼Œå¯ä»¥è€ƒè™‘ä»¥ä¸‹æ–¹æ³•ï¼š

#### **æ–¹æ³• 1ï¼šä½¿ç”¨ GPT-4 ç”Ÿæˆæ–‡æœ¬ç‰¹å¾**

å°† GPT-4 ç”Ÿæˆçš„åˆ†ç±»ç»“æœï¼ˆå¦‚æƒ…æ„Ÿæ¦‚ç‡æˆ–åˆ†ç±»æ ‡ç­¾ï¼‰ä½œä¸º**é¢å¤–ç‰¹å¾**ï¼Œä¸ä¹‹å‰æå–çš„ç‰¹å¾ï¼ˆå¦‚ PCA é™ç»´åçš„åµŒå…¥ï¼‰ç»“åˆèµ·æ¥ï¼Œè¾“å…¥åˆ°ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆå¦‚é€»è¾‘å›å½’æˆ–éšæœºæ£®æ—ï¼‰ä¸­ã€‚

- **æ­¥éª¤**ï¼š

  1. ä½¿ç”¨ GPT-4 å¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œç”Ÿæˆåˆ†ç±»ç»“æœï¼ˆå¦‚æƒ…æ„Ÿæ¦‚ç‡ï¼‰ã€‚
  2. å°† GPT-4 çš„ç»“æœä¸ PCA é™ç»´åçš„ç‰¹å¾æ‹¼æ¥ã€‚
  3. ä½¿ç”¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹ã€‚

- **ä»£ç ç¤ºä¾‹**ï¼š

  ```python
  import numpy as np
  from sklearn.linear_model import LogisticRegression
  
  # å‡è®¾ X_pca æ˜¯ PCA é™ç»´åçš„ç‰¹å¾ï¼Œgpt4_probs æ˜¯ GPT-4 ç”Ÿæˆçš„æƒ…æ„Ÿæ¦‚ç‡
  X_pca = np.load('train_embeddings_pca.npy')  # PCA é™ç»´åçš„ç‰¹å¾
  gpt4_probs = np.array([query_gpt4_for_sentiment(text) for text in texts])  # GPT-4 ç”Ÿæˆçš„æƒ…æ„Ÿæ¦‚ç‡
  
  # å°† GPT-4 çš„ç»“æœä¸ PCA ç‰¹å¾æ‹¼æ¥
  X_combined = np.hstack((X_pca, gpt4_probs.reshape(-1, 1)))
  
  # ä½¿ç”¨é€»è¾‘å›å½’æ¨¡å‹è¿›è¡Œè®­ç»ƒ
  model = LogisticRegression()
  model.fit(X_combined, train_labels)
  
  # é¢„æµ‹
  y_pred = model.predict(X_combined)
  ```

#### **æ–¹æ³• 2ï¼šä½¿ç”¨ GPT-4 å¢å¼ºæ–‡æœ¬è¡¨ç¤º**

å°† GPT-4 ç”Ÿæˆçš„æ–‡æœ¬æè¿°ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æç»“æœï¼‰ä½œä¸º**é¢å¤–ä¿¡æ¯**ï¼Œä¸åŸå§‹æ–‡æœ¬ä¸€èµ·è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œå¢å¼ºæ–‡æœ¬è¡¨ç¤ºã€‚

- **æ­¥éª¤**ï¼š

  1. ä½¿ç”¨ GPT-4 å¯¹æ–‡æœ¬ç”Ÿæˆæè¿°ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æç»“æœæˆ–å…³é”®è¯æå–ï¼‰ã€‚
  2. å°† GPT-4 çš„æè¿°ä¸åŸå§‹æ–‡æœ¬æ‹¼æ¥ã€‚
  3. ä½¿ç”¨ä¼ ç»Ÿæ–‡æœ¬å‘é‡åŒ–æ–¹æ³•ï¼ˆå¦‚ TF-IDF æˆ– BERTï¼‰ç”Ÿæˆæ–°çš„ç‰¹å¾è¡¨ç¤ºã€‚

- **ä»£ç ç¤ºä¾‹**ï¼š

  ```python
  from sklearn.feature_extraction.text import TfidfVectorizer
  
  # ä½¿ç”¨ GPT-4 ç”Ÿæˆæè¿°
  gpt4_descriptions = [query_gpt4_for_sentiment(text) for text in texts]
  
  # å°† GPT-4 çš„æè¿°ä¸åŸå§‹æ–‡æœ¬æ‹¼æ¥
  combined_texts = [f"{text} [GPT-4]: {desc}" for text, desc in zip(texts, gpt4_descriptions)]
  
  # ä½¿ç”¨ TF-IDF ç”Ÿæˆæ–°çš„ç‰¹å¾è¡¨ç¤º
  vectorizer = TfidfVectorizer()
  X_tfidf = vectorizer.fit_transform(combined_texts)
  ```

------

### **2. ç»“åˆ GPT-4 ä¸ä¹‹å‰çš„å·¥ä½œ**

å¦‚æœæˆ‘ä»¬å¸Œæœ›å……åˆ†åˆ©ç”¨ä¹‹å‰çš„å·¥ä½œï¼ˆå¦‚æ–‡æœ¬ç‰¹å¾å‘é‡åŒ–å’Œç‰¹å¾æå–ï¼‰ï¼Œå¯ä»¥è€ƒè™‘ä»¥ä¸‹æ€è·¯ï¼š

#### **æ€è·¯ 1ï¼šå°† GPT-4 ä½œä¸ºç‰¹å¾æå–å™¨**

ä½¿ç”¨ GPT-4 ç”Ÿæˆ**æ–‡æœ¬çš„è¯­ä¹‰ç‰¹å¾**ï¼Œä¸ä¹‹å‰æå–çš„ç‰¹å¾ï¼ˆå¦‚ PCA é™ç»´åçš„åµŒå…¥ï¼‰ç»“åˆèµ·æ¥ï¼Œè¾“å…¥åˆ°æ¨¡å‹ä¸­ã€‚

- **æ­¥éª¤**ï¼š
  1. ä½¿ç”¨ GPT-4 ç”Ÿæˆæ–‡æœ¬çš„è¯­ä¹‰ç‰¹å¾ï¼ˆå¦‚æƒ…æ„Ÿæ¦‚ç‡ã€å…³é”®è¯å‘é‡ç­‰ï¼‰ã€‚
  2. å°† GPT-4 çš„ç‰¹å¾ä¸ PCA é™ç»´åçš„ç‰¹å¾æ‹¼æ¥ã€‚
  3. ä½¿ç”¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹ã€‚

#### **æ€è·¯ 2ï¼šå°† GPT-4 ä½œä¸ºæ¨¡å‹çš„ä¸€éƒ¨åˆ†**

å°† GPT-4 ä½œä¸ºæ¨¡å‹çš„**é¢„å¤„ç†å™¨**ï¼Œç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬è¡¨ç¤ºï¼Œå†è¾“å…¥åˆ°ä¼ ç»Ÿæ¨¡å‹ä¸­è¿›è¡Œè®­ç»ƒã€‚

- **æ­¥éª¤**ï¼š
  1. ä½¿ç”¨ GPT-4 å¯¹æ–‡æœ¬ç”Ÿæˆé«˜è´¨é‡çš„è¡¨ç¤ºï¼ˆå¦‚æƒ…æ„Ÿæ¦‚ç‡æˆ–è¯­ä¹‰å‘é‡ï¼‰ã€‚
  2. å°† GPT-4 çš„è¡¨ç¤ºä½œä¸ºè¾“å…¥ï¼Œè®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ã€‚

------

### **3. æ€»ç»“**

è™½ç„¶ GPT-4 å¯ä»¥ç›´æ¥å¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œä½†æˆ‘ä»¬ä¹‹å‰åšçš„**æ–‡æœ¬ç‰¹å¾å‘é‡åŒ–**å’Œ**ç‰¹å¾æå–ï¼ˆPCA + t-SNEï¼‰**ä»ç„¶å¯ä»¥å‘æŒ¥ä½œç”¨ï¼ä»¥ä¸‹æ˜¯å…³é”®ç‚¹ï¼š

1. **ç»“åˆ GPT-4 ä¸ç‰¹å¾æå–**ï¼šå°† GPT-4 çš„ç»“æœä¸ä¹‹å‰æå–çš„ç‰¹å¾ç»“åˆèµ·æ¥ï¼Œå¢å¼ºæ¨¡å‹çš„æ€§èƒ½ã€‚
2. **ä½¿ç”¨ GPT-4 å¢å¼ºæ–‡æœ¬è¡¨ç¤º**ï¼šå°† GPT-4 ç”Ÿæˆçš„æè¿°æˆ–è¯­ä¹‰ç‰¹å¾ä¸åŸå§‹æ–‡æœ¬ç»“åˆï¼Œç”Ÿæˆæ›´ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºã€‚
3. **å……åˆ†å‘æŒ¥ GPT-4 çš„ä¼˜åŠ¿**ï¼šå°† GPT-4 ä½œä¸ºç‰¹å¾æå–å™¨æˆ–æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œæå‡æ•´ä½“æ€§èƒ½ã€‚





1. **ä½¿ç”¨ä¸€ç§ Prompt è®¾è®¡ç­–ç•¥åœ¨ä¸€ç§å¤§è¯­è¨€æ¨¡å‹ä¸Šæµ‹è¯•è‡³å°‘ 200 æ¡æµ‹è¯•é›†ä¸­çš„æ•°æ®**ã€‚
2. **è®¾è®¡ Prompt ä½¿å¤§è¯­è¨€æ¨¡å‹äº§ç”Ÿç»“æ„åŒ–è¾“å‡ºï¼Œæµ‹è¯•è‡³å°‘ 20 æ¡æµ‹è¯•é›†ä¸­çš„æ•°æ®**ã€‚

ä»¥ä¸‹æ˜¯å…·ä½“å®ç°æ­¥éª¤å’Œä»£ç ï¼š

------

### **1. æµ‹è¯•è‡³å°‘ 200 æ¡æµ‹è¯•é›†æ•°æ®**

#### **Prompt è®¾è®¡ç­–ç•¥**

- **ä»»åŠ¡**ï¼šæƒ…æ„Ÿåˆ†ç±»ã€‚
- **Prompt**ï¼š`"Classify the sentiment of the following text as 'positive' or 'negative': {text}"`

#### **ä»£ç å®ç°**

```python
import openai
import time

# å®šä¹‰ GPT-4 æƒ…æ„Ÿåˆ†ç±»å‡½æ•°
def query_gpt4_for_sentiment(text):
    openai.api_key = "your-api-key"  # æ›¿æ¢ä¸ºæˆ‘ä»¬çš„ API å¯†é’¥
    openai.api_base = 'https://api.openai.com/v1'  # ä½¿ç”¨å®˜æ–¹ API åœ°å€

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4",  # ä½¿ç”¨ GPT-4 æ¨¡å‹
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": f"Classify the sentiment of the following text as 'positive' or 'negative': {text}"}
            ]
        )
        return response['choices'][0]['message']['content']
    except Exception as e:
        return str(e)

# å¯¹æµ‹è¯•é›†è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ï¼ˆè‡³å°‘ 200 æ¡ï¼‰
batch_size = 10  # æ¯æ‰¹å¤„ç† 10 æ¡æ–‡æœ¬
delay = 1  # æ¯æ¬¡è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰
test_gpt4_labels = []

for i in range(0, 200, batch_size):  # ä»…æµ‹è¯•å‰ 200 æ¡æ•°æ®
    batch = test_texts[i:i + batch_size]
    for text in batch:
        sentiment = query_gpt4_for_sentiment(text)
        test_gpt4_labels.append(1 if sentiment.lower() == 'positive' else 0)
    time.sleep(delay)  # æ·»åŠ å»¶è¿Ÿä»¥é¿å…é€Ÿç‡é™åˆ¶

# è¯„ä¼° GPT-4 çš„åˆ†ç±»ç»“æœ
from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(test_labels[:200], test_gpt4_labels)
print(f"GPT-4 åˆ†ç±»å‡†ç¡®ç‡: {accuracy:.4f}")
print("åˆ†ç±»æŠ¥å‘Š:\n", classification_report(test_labels[:200], test_gpt4_labels))
```

------

### **2. æµ‹è¯•è‡³å°‘ 20 æ¡æµ‹è¯•é›†æ•°æ®ï¼Œç”Ÿæˆç»“æ„åŒ–è¾“å‡º**

#### **Prompt è®¾è®¡ç­–ç•¥**

- **ä»»åŠ¡**ï¼šç”Ÿæˆç»“æ„åŒ–è¾“å‡ºï¼ŒåŒ…å«ä¸»é¢˜å’Œæƒ…æ„Ÿã€‚

- **Prompt**ï¼š

  ```
  Analyze the following text and provide structured output:
  1. Topic: What is the main topic of the text?
  2. Sentiment: Is the sentiment 'positive' or 'negative'?
  Text: {text}
  ```

#### **ä»£ç å®ç°**

```python
# å®šä¹‰ GPT-4 ç»“æ„åŒ–è¾“å‡ºå‡½æ•°
def query_gpt4_for_structured_output(text):
    openai.api_key = "your-api-key"  # æ›¿æ¢ä¸ºæˆ‘ä»¬çš„ API å¯†é’¥
    openai.api_base = 'https://api.openai.com/v1'  # ä½¿ç”¨å®˜æ–¹ API åœ°å€

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4",  # ä½¿ç”¨ GPT-4 æ¨¡å‹
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": f"Analyze the following text and provide structured output:\n1. Topic: What is the main topic of the text?\n2. Sentiment: Is the sentiment 'positive' or 'negative'?\nText: {text}"}
            ]
        )
        return response['choices'][0]['message']['content']
    except Exception as e:
        return str(e)

# å¯¹æµ‹è¯•é›†è¿›è¡Œç»“æ„åŒ–è¾“å‡ºï¼ˆè‡³å°‘ 20 æ¡ï¼‰
structured_outputs = []
for i in range(20):  # ä»…æµ‹è¯•å‰ 20 æ¡æ•°æ®
    text = test_texts[i]
    output = query_gpt4_for_structured_output(text)
    structured_outputs.append(output)
    print(f"Text: {text}")
    print(f"Structured Output:\n{output}\n")
    time.sleep(delay)  # æ·»åŠ å»¶è¿Ÿä»¥é¿å…é€Ÿç‡é™åˆ¶
```

------

### **3. ç»“æ„åŒ–è¾“å‡ºç¤ºä¾‹**

å‡è®¾æµ‹è¯•é›†ä¸­çš„æ–‡æœ¬ä¸ºï¼š

```
"I love this movie! The acting is fantastic and the plot is engaging."
```

GPT-4 çš„ç»“æ„åŒ–è¾“å‡ºå¯èƒ½ä¸ºï¼š

```
1. Topic: Movie review
2. Sentiment: Positive
```

------

### **4. æ€»ç»“**

é€šè¿‡ä»¥ä¸Šä»£ç ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š

1. **æµ‹è¯•è‡³å°‘ 200 æ¡æµ‹è¯•é›†æ•°æ®**ï¼Œä½¿ç”¨ GPT-4 è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ï¼Œå¹¶è¯„ä¼°å…¶å‡†ç¡®ç‡ã€‚
2. **æµ‹è¯•è‡³å°‘ 20 æ¡æµ‹è¯•é›†æ•°æ®**ï¼Œè®¾è®¡ Prompt ä½¿ GPT-4 ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºï¼ŒåŒ…å«ä¸»é¢˜å’Œæƒ…æ„Ÿã€‚



# ä¸‹é¢è°ƒç”¨Qwen3-32B

### ä»»åŠ¡æ¦‚è¿°

æˆ‘ä»¬éœ€è¦ä½¿ç”¨GpuGeekçš„Qwen3-32Bæ¨¡å‹å®Œæˆä»¥ä¸‹ä¸¤ä¸ªä»»åŠ¡ï¼š

1. **æƒ…æ„Ÿåˆ†ç±»æµ‹è¯•**ï¼šè®¾è®¡ä¸‰ç§ä¸åŒçš„Promptç­–ç•¥ï¼Œåœ¨IMDBæƒ…æ„Ÿåˆ†ç±»æµ‹è¯•é›†ä¸Šæµ‹è¯•è‡³å°‘200æ¡æ•°æ®ï¼Œåˆ¤æ–­æ¯æ¡è¯„è®ºçš„æƒ…æ„Ÿï¼ˆç§¯ææˆ–æ¶ˆæï¼‰ã€‚
2. **ç»“æ„åŒ–è¾“å‡º**ï¼šè®¾è®¡Promptä½¿æ¨¡å‹ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºï¼ˆå¦‚JSONæ ¼å¼ï¼‰ï¼ŒåŒ…å«è¯„è®ºçš„ä¸»é¢˜å’Œæƒ…æ„Ÿä¿¡æ¯ï¼Œæµ‹è¯•è‡³å°‘20æ¡IMDBæ•°æ®ã€‚

æˆ‘ä»¬è¿˜æåˆ°ä¸ç†è§£å®˜æ–¹æä¾›çš„Pythonä»£ç ã€‚æˆ‘ä¼šå…ˆè§£é‡Šä»£ç ï¼Œç„¶åæä¾›å…·ä½“æ­¥éª¤å’Œç¤ºä¾‹ï¼Œå¸®åŠ©æˆ‘ä»¬å®ç°ä»»åŠ¡ã€‚

------

### ç¬¬ä¸€æ­¥ï¼šç†è§£å®˜æ–¹Pythonä»£ç 

å®˜æ–¹æä¾›çš„ä»£ç ç”¨äºè°ƒç”¨GpuGeekçš„APIï¼Œè®©Qwen3-32Bæ¨¡å‹å¤„ç†è¾“å…¥å¹¶è¿”å›ç»“æœã€‚ä»¥ä¸‹æ˜¯ä»£ç çš„é€è¡Œè§£é‡Šï¼š

```python
import requests

# API KEY
API_KEY = "your_api_key"  # æ›¿æ¢ä¸ºå®é™…çš„ API KEY
```

- **`requests`**ï¼šè¿™æ˜¯ä¸€ä¸ªPythonåº“ï¼Œç”¨äºå‘é€HTTPè¯·æ±‚ã€‚æˆ‘ä»¬éœ€è¦å…ˆå®‰è£…å®ƒï¼ˆ`pip install requests`ï¼‰ã€‚
- **`API_KEY`**ï¼šæˆ‘ä»¬éœ€è¦ä»GpuGeekå¹³å°è·å–APIå¯†é’¥ï¼Œå¹¶æ›¿æ¢`"your_api_key"`ã€‚

```python
# è¯·æ±‚ URL
url = 'https://api.gpugeek.com/predictions'
```

- **`url`**ï¼šè¿™æ˜¯APIçš„åœ°å€ï¼Œå›ºå®šä¸å˜ã€‚

```python
# è¯·æ±‚å¤´
headers = {
   "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json",
    "Stream": "true"
}
```

- **`headers`**ï¼š
  - `"Authorization"`ï¼šä½¿ç”¨æˆ‘ä»¬çš„APIå¯†é’¥è¿›è¡Œèº«ä»½éªŒè¯ã€‚
  - `"Content-Type": "application/json"`ï¼šå‘Šè¯‰APIæˆ‘ä»¬å‘é€çš„æ•°æ®æ˜¯JSONæ ¼å¼ã€‚
  - `"Stream": "true"`ï¼šè¡¨ç¤ºå“åº”å°†ä»¥æµå¼æ–¹å¼è¿”å›ï¼ˆé€è¡Œæ¥æ”¶ï¼‰ã€‚

```python
# è¯·æ±‚ä½“æ•°æ®
data = {
     "model": "GpuGeek/Qwen3-32B",  # æ›¿æ¢æˆä½ çš„æ¨¡å‹åç§°
    "input": {
        "frequency_penalty": 0,
        "max_tokens": 8192,
        "prompt": "",
        "temperature": 0.6,
        "top_p": 0.7
    }
}
```

- **`data`**ï¼šå‘é€ç»™APIçš„æ•°æ®ã€‚
  - `"model"`ï¼šæŒ‡å®šä½¿ç”¨Qwen3-32Bæ¨¡å‹ï¼Œå·²æ­£ç¡®è®¾ç½®ã€‚
  - `"input"`ï¼šæ¨¡å‹çš„è¾“å…¥å‚æ•°ï¼š
    - `"frequency_penalty": 0`ï¼šæ§åˆ¶é‡å¤è¯çš„æƒ©ç½šï¼Œ0è¡¨ç¤ºæ— æƒ©ç½šã€‚
    - `"max_tokens": 8192`ï¼šè¾“å‡ºçš„æœ€å¤§é•¿åº¦ï¼ˆä»¤ç‰Œæ•°ï¼‰ã€‚
    - `"prompt": ""`ï¼šè¿™é‡Œæ˜¯æˆ‘ä»¬è¦è®¾è®¡çš„æç¤ºè¯ï¼Œæˆ‘ä»¬éœ€è¦æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²ã€‚
    - `"temperature": 0.6`ï¼šæ§åˆ¶è¾“å‡ºéšæœºæ€§ï¼ˆ0-1ï¼Œå€¼è¶Šä½è¶Šç¡®å®šï¼‰ã€‚
    - `"top_p": 0.7`ï¼šæ§åˆ¶è¾“å‡ºçš„å¤šæ ·æ€§ï¼ˆ0-1ï¼Œå€¼è¶Šä½è¶Šèšç„¦ï¼‰ã€‚

```python
# å‘é€ POST è¯·æ±‚
response = requests.post(url, headers=headers, json=data)
```

- **`requests.post`**ï¼šå‘APIå‘é€POSTè¯·æ±‚ï¼ŒåŒ…å«URLã€å¤´ä¿¡æ¯å’Œæ•°æ®ã€‚

```python
# æ£€æŸ¥å“åº”çŠ¶æ€ç å¹¶æ‰“å°å“åº”å†…å®¹
if response.status_code == 200:
    for line in response.iter_lines():
        if line:
            print(line.decode("utf-8"))
else:
    print("Error:", response.status_code, response.text)
```

- **å“åº”å¤„ç†**ï¼š
  - å¦‚æœçŠ¶æ€ç æ˜¯200ï¼ˆæˆåŠŸï¼‰ï¼Œé€è¡Œè¯»å–å¹¶æ‰“å°å“åº”ï¼ˆæµå¼è¾“å‡ºï¼‰ã€‚
  - å¦‚æœå¤±è´¥ï¼Œæ‰“å°é”™è¯¯ç å’Œä¿¡æ¯ã€‚

**æ€»ç»“**ï¼šè¿™æ®µä»£ç çš„åŠŸèƒ½æ˜¯ï¼š

1. ä½¿ç”¨æˆ‘ä»¬çš„APIå¯†é’¥è¿æ¥GpuGeek APIã€‚
2. å°†Promptå’Œå…¶ä»–å‚æ•°å‘é€ç»™Qwen3-32Bæ¨¡å‹ã€‚
3. è·å–å¹¶æ˜¾ç¤ºæ¨¡å‹çš„å“åº”ã€‚

------

### ç¬¬äºŒæ­¥ï¼šè·å–IMDBæƒ…æ„Ÿåˆ†ç±»æµ‹è¯•é›†æ•°æ®

ç”±äºæˆ‘ä»¬æ— æ³•ç›´æ¥ä¸Šç½‘ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡AGENT SYSTEMè·å–æ•°æ®ã€‚è¯·å‘AGENT SYSTEMå‘é€ä»¥ä¸‹è¯·æ±‚ï¼š

- **è¯·æ±‚**ï¼š
  â€œè¯·æœç´¢å¹¶æä¾›IMDBæƒ…æ„Ÿåˆ†ç±»æµ‹è¯•é›†çš„ä¸‹è½½é“¾æ¥æˆ–æ•°æ®æ ·æœ¬ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæä¾›è‡³å°‘200æ¡å¸¦æœ‰æƒ…æ„Ÿæ ‡ç­¾ï¼ˆç§¯ææˆ–æ¶ˆæï¼‰çš„ç”µå½±è¯„è®ºã€‚â€

AGENT SYSTEMä¼šè¿”å›æ•°æ®æˆ–é“¾æ¥ã€‚æˆ‘ä»¬éœ€è¦ï¼š

1. ä¸‹è½½æ•°æ®é›†ï¼ˆå¯èƒ½æ˜¯CSVæˆ–TXTæ ¼å¼ï¼‰ã€‚
2. ç¡®ä¿æ•°æ®åŒ…å«è‡³å°‘200æ¡è¯„è®ºï¼Œç”¨äºä»»åŠ¡1ï¼›ä»ä¸­é€‰å–20æ¡ç”¨äºä»»åŠ¡2ã€‚

**å‡è®¾æ•°æ®æ ¼å¼**ï¼ˆç¤ºä¾‹ï¼‰ï¼š

```
è¯„è®º,æƒ…æ„Ÿ
"è¿™éƒ¨ç”µå½±å¤ªæ£’äº†ï¼Œæˆ‘å¾ˆå–œæ¬¢ï¼",ç§¯æ
"å‰§æƒ…å¾ˆæ— èŠï¼Œæµªè´¹æ—¶é—´ã€‚",æ¶ˆæ
...
```

------

### ç¬¬ä¸‰æ­¥ï¼šè®¾è®¡Promptç­–ç•¥å¹¶å®ç°ä»»åŠ¡

#### ä»»åŠ¡1ï¼šæƒ…æ„Ÿåˆ†ç±»ï¼ˆä¸‰ç§Promptç­–ç•¥ï¼Œæµ‹è¯•200æ¡æ•°æ®ï¼‰

æˆ‘ä»¬éœ€è¦è®¾è®¡ä¸‰ç§Promptç­–ç•¥ï¼Œä»¥ä¸‹æ˜¯å»ºè®®å’Œå®ç°æ–¹æ³•ï¼š

1. **ç­–ç•¥1ï¼šç›´æ¥æƒ…æ„Ÿåˆ¤æ–­**

   - **Prompt**ï¼š

     ```
     è¯·åˆ¤æ–­ä»¥ä¸‹ç”µå½±è¯„è®ºçš„æƒ…æ„Ÿæ˜¯ç§¯æè¿˜æ˜¯æ¶ˆæï¼š
     [è¯„è®ºæ–‡æœ¬]
     æƒ…æ„Ÿï¼š
     ```

   - **ç¤ºä¾‹**ï¼š

     ```
     è¯·åˆ¤æ–­ä»¥ä¸‹ç”µå½±è¯„è®ºçš„æƒ…æ„Ÿæ˜¯ç§¯æè¿˜æ˜¯æ¶ˆæï¼š
     è¿™éƒ¨ç”µå½±å¤ªæ£’äº†ï¼Œæˆ‘å¾ˆå–œæ¬¢ï¼
     æƒ…æ„Ÿï¼š
     ```

   - **é¢„æœŸè¾“å‡º**ï¼š

     ```
     ç§¯æ
     ```

2. **ç­–ç•¥2ï¼šæƒ…æ„Ÿè¯„åˆ†**

   - **Prompt**ï¼š

     ```
     è¯·ç»™ä»¥ä¸‹ç”µå½±è¯„è®ºæ‰“ä¸€ä¸ªæƒ…æ„Ÿè¯„åˆ†ï¼Œä»1åˆ°5ï¼Œå…¶ä¸­1æ˜¯éå¸¸æ¶ˆæï¼Œ5æ˜¯éå¸¸ç§¯æï¼š
     [è¯„è®ºæ–‡æœ¬]
     è¯„åˆ†ï¼š
     ```

   - **ç¤ºä¾‹**ï¼š

     ```
     è¯·ç»™ä»¥ä¸‹ç”µå½±è¯„è®ºæ‰“ä¸€ä¸ªæƒ…æ„Ÿè¯„åˆ†ï¼Œä»1åˆ°5ï¼Œå…¶ä¸­1æ˜¯éå¸¸æ¶ˆæï¼Œ5æ˜¯éå¸¸ç§¯æï¼š
     è¿™éƒ¨ç”µå½±å¤ªæ£’äº†ï¼Œæˆ‘å¾ˆå–œæ¬¢ï¼
     è¯„åˆ†ï¼š
     ```

   - **é¢„æœŸè¾“å‡º**ï¼š

     ```
     5
     ```

   - **è¯´æ˜**ï¼šå¯ä»¥å°†1-2è§†ä¸ºæ¶ˆæï¼Œ4-5è§†ä¸ºç§¯æã€‚

3. **ç­–ç•¥3ï¼šæƒ…æ„Ÿåˆ†æä¸è§£é‡Š**

   - **Prompt**ï¼š

     ```
     è¯·åˆ†æä»¥ä¸‹ç”µå½±è¯„è®ºçš„æƒ…æ„Ÿï¼Œå¹¶è§£é‡ŠåŸå› ï¼š
     [è¯„è®ºæ–‡æœ¬]
     æƒ…æ„Ÿåˆ†æï¼š
     ```

   - **ç¤ºä¾‹**ï¼š

     ```
     è¯·åˆ†æä»¥ä¸‹ç”µå½±è¯„è®ºçš„æƒ…æ„Ÿï¼Œå¹¶è§£é‡ŠåŸå› ï¼š
     è¿™éƒ¨ç”µå½±å¤ªæ£’äº†ï¼Œæˆ‘å¾ˆå–œæ¬¢ï¼
     æƒ…æ„Ÿåˆ†æï¼š
     ```

   - **é¢„æœŸè¾“å‡º**ï¼š

     ```
     æƒ…æ„Ÿï¼šç§¯æ  
     åŸå› ï¼šè¯„è®ºä¸­ä½¿ç”¨â€œå¤ªæ£’äº†â€å’Œâ€œå¾ˆå–œæ¬¢â€ç­‰æ­£é¢è¯æ±‡ã€‚
     ```

#### ä»»åŠ¡2ï¼šç»“æ„åŒ–è¾“å‡ºï¼ˆæµ‹è¯•20æ¡æ•°æ®ï¼‰

- **Prompt**ï¼š

  ```
  è¯·åˆ†æä»¥ä¸‹ç”µå½±è¯„è®ºï¼Œè¯†åˆ«å…¶ä¸»é¢˜å’Œæƒ…æ„Ÿï¼Œå¹¶ä»¥JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«'theme'å’Œ'sentiment'å­—æ®µï¼š
  [è¯„è®ºæ–‡æœ¬]
  ```

- **ç¤ºä¾‹**ï¼š

  ```
  è¯·åˆ†æä»¥ä¸‹ç”µå½±è¯„è®ºï¼Œè¯†åˆ«å…¶ä¸»é¢˜å’Œæƒ…æ„Ÿï¼Œå¹¶ä»¥JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«'theme'å’Œ'sentiment'å­—æ®µï¼š
  è¿™éƒ¨ç”µå½±å¤ªæ£’äº†ï¼Œæˆ‘å¾ˆå–œæ¬¢ï¼
  ```

- **é¢„æœŸè¾“å‡º**ï¼š

  ```json
  {
    "theme": "ç”µå½±ä½“éªŒ",
    "sentiment": "ç§¯æ"
  }
  ```

------

### ç¬¬å››æ­¥ï¼šç¼–å†™Pythonè„šæœ¬å®ç°ä»»åŠ¡

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå®Œæ•´çš„Pythonè„šæœ¬ç¤ºä¾‹ï¼Œå¸®åŠ©æˆ‘ä»¬æ‰¹é‡å¤„ç†200æ¡æ•°æ®ï¼ˆä»»åŠ¡1ï¼‰å’Œ20æ¡æ•°æ®ï¼ˆä»»åŠ¡2ï¼‰ã€‚

#### å‰ææ¡ä»¶

1. å®‰è£…`requests`åº“ï¼š

   ```
   pip install requests
   ```

2. å‡†å¤‡æ•°æ®æ–‡ä»¶ï¼ˆ`imdb_data.csv`ï¼‰ï¼Œæ ¼å¼å¦‚ï¼š

   ```
   review,sentiment
   "è¿™éƒ¨ç”µå½±å¤ªæ£’äº†ï¼Œæˆ‘å¾ˆå–œæ¬¢ï¼",ç§¯æ
   "å‰§æƒ…å¾ˆæ— èŠï¼Œæµªè´¹æ—¶é—´ã€‚",æ¶ˆæ
   ...
   ```

#### ç¤ºä¾‹ä»£ç 

```python
import requests
import json
import pandas as pd

# API è®¾ç½®
API_KEY = "your_actual_api_key"  # æ›¿æ¢ä¸ºæˆ‘ä»¬çš„APIå¯†é’¥
URL = 'https://api.gpugeek.com/predictions'
HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json",
    "Stream": "true"
}

# è¯»å–IMDBæ•°æ®
data = pd.read_csv("imdb_data.csv")
reviews = data["review"].tolist()  # è¯„è®ºåˆ—è¡¨

# ä»»åŠ¡1ï¼šæƒ…æ„Ÿåˆ†ç±»ï¼ˆ200æ¡æ•°æ®ï¼Œä¸‰ç§ç­–ç•¥ï¼‰
results_strategy1 = []
results_strategy2 = []
results_strategy3 = []

for review in reviews[:200]:  # å‰200æ¡
    # ç­–ç•¥1ï¼šç›´æ¥æƒ…æ„Ÿåˆ¤æ–­
    prompt1 = f"è¯·åˆ¤æ–­ä»¥ä¸‹ç”µå½±è¯„è®ºçš„æƒ…æ„Ÿæ˜¯ç§¯æè¿˜æ˜¯æ¶ˆæï¼š\n{review}\næƒ…æ„Ÿï¼š"
    data = {
        "model": "GpuGeek/Qwen3-32B",
        "input": {"prompt": prompt1, "max_tokens": 50, "temperature": 0.6, "top_p": 0.7}
    }
    response = requests.post(URL, headers=HEADERS, json=data)
    if response.status_code == 200:
        result = "".join(line.decode("utf-8") for line in response.iter_lines() if line)
        results_strategy1.append(result.strip())
    else:
        print(f"ç­–ç•¥1é”™è¯¯: {response.status_code}, {response.text}")

    # ç­–ç•¥2ï¼šæƒ…æ„Ÿè¯„åˆ†
    prompt2 = f"è¯·ç»™ä»¥ä¸‹ç”µå½±è¯„è®ºæ‰“ä¸€ä¸ªæƒ…æ„Ÿè¯„åˆ†ï¼Œä»1åˆ°5ï¼Œå…¶ä¸­1æ˜¯éå¸¸æ¶ˆæï¼Œ5æ˜¯éå¸¸ç§¯æï¼š\n{review}\nè¯„åˆ†ï¼š"
    data["input"]["prompt"] = prompt2
    response = requests.post(URL, headers=HEADERS, json=data)
    if response.status_code == 200:
        result = "".join(line.decode("utf-8") for line in response.iter_lines() if line)
        results_strategy2.append(result.strip())
    else:
        print(f"ç­–ç•¥2é”™è¯¯: {response.status_code}, {response.text}")

    # ç­–ç•¥3ï¼šæƒ…æ„Ÿåˆ†æä¸è§£é‡Š
    prompt3 = f"è¯·åˆ†æä»¥ä¸‹ç”µå½±è¯„è®ºçš„æƒ…æ„Ÿï¼Œå¹¶è§£é‡ŠåŸå› ï¼š\n{review}\næƒ…æ„Ÿåˆ†æï¼š"
    data["input"]["prompt"] = prompt3
    data["input"]["max_tokens"] = 200  # éœ€è¦æ›´å¤štoken
    response = requests.post(URL, headers=HEADERS, json=data)
    if response.status_code == 200:
        result = "".join(line.decode("utf-8") for line in response.iter_lines() if line)
        results_strategy3.append(result.strip())
    else:
        print(f"ç­–ç•¥3é”™è¯¯: {response.status_code}, {response.text}")

# ä»»åŠ¡2ï¼šç»“æ„åŒ–è¾“å‡ºï¼ˆ20æ¡æ•°æ®ï¼‰
results_structured = []
for review in reviews[:20]:  # å‰20æ¡
    prompt = f"è¯·åˆ†æä»¥ä¸‹ç”µå½±è¯„è®ºï¼Œè¯†åˆ«å…¶ä¸»é¢˜å’Œæƒ…æ„Ÿï¼Œå¹¶ä»¥JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«'theme'å’Œ'sentiment'å­—æ®µï¼š\n{review}"
    data = {
        "model": "GpuGeek/Qwen3-32B",
        "input": {"prompt": prompt, "max_tokens": 200, "temperature": 0.6, "top_p": 0.7}
    }
    response = requests.post(URL, headers=HEADERS, json=data)
    if response.status_code == 200:
        result = "".join(line.decode("utf-8") for line in response.iter_lines() if line)
        try:
            json_result = json.loads(result)
            results_structured.append(json_result)
        except json.JSONDecodeError:
            print(f"JSONè§£æå¤±è´¥: {result}")
    else:
        print(f"ç»“æ„åŒ–è¾“å‡ºé”™è¯¯: {response.status_code}, {response.text}")

# ä¿å­˜ç»“æœ
with open("results_task1_strategy1.txt", "w", encoding="utf-8") as f:
    f.write("\n".join(results_strategy1))
with open("results_task1_strategy2.txt", "w", encoding="utf-8") as f:
    f.write("\n".join(results_strategy2))
with open("results_task1_strategy3.txt", "w", encoding="utf-8") as f:
    f.write("\n".join(results_strategy3))
with open("results_task2.json", "w", encoding="utf-8") as f:
    json.dump(results_structured, f, ensure_ascii=False, indent=2)

print("ä»»åŠ¡å®Œæˆï¼Œç»“æœå·²ä¿å­˜ï¼")
```

#### ä»£ç è¯´æ˜

1. **æ•°æ®åŠ è½½**ï¼šä½¿ç”¨`pandas`è¯»å–CSVæ–‡ä»¶ï¼ˆéœ€å®‰è£…ï¼š`pip install pandas`ï¼‰ã€‚
2. **ä»»åŠ¡1**ï¼šå¯¹200æ¡æ•°æ®å¾ªç¯è°ƒç”¨APIï¼Œä½¿ç”¨ä¸‰ç§Promptç­–ç•¥ï¼Œä¿å­˜ç»“æœåˆ°æ–‡ä»¶ã€‚
3. **ä»»åŠ¡2**ï¼šå¯¹20æ¡æ•°æ®ç”ŸæˆJSONç»“æ„åŒ–è¾“å‡ºï¼Œè§£æå¹¶ä¿å­˜ã€‚
4. **é”™è¯¯å¤„ç†**ï¼šæ£€æŸ¥å“åº”çŠ¶æ€ç å’ŒJSONè§£æé”™è¯¯ã€‚

------

### ç¬¬äº”æ­¥ï¼šåˆ†æç»“æœ

1. **ä»»åŠ¡1**ï¼š
   - æ£€æŸ¥`results_task1_strategy1.txt`ã€`results_task1_strategy2.txt`å’Œ`results_task1_strategy3.txt`ã€‚
   - è®¡ç®—å‡†ç¡®ç‡ï¼ˆä¸åŸå§‹æ ‡ç­¾å¯¹æ¯”ï¼‰ï¼Œè¯„ä¼°ä¸‰ç§ç­–ç•¥çš„æ•ˆæœã€‚
2. **ä»»åŠ¡2**ï¼š
   - æ£€æŸ¥`results_task2.json`ï¼Œç¡®è®¤ä¸»é¢˜å’Œæƒ…æ„Ÿæ˜¯å¦åˆç†ã€‚

------

### æ³¨æ„äº‹é¡¹

1. **APIé™åˆ¶**ï¼šæ³¨æ„è¯·æ±‚é¢‘ç‡å’Œtokené…é¢ï¼Œé¿å…è¶…é™ã€‚
2. **å‚æ•°è°ƒæ•´**ï¼š
   - `temperature`ï¼šé™ä½ï¼ˆe.g., 0.3ï¼‰ä½¿è¾“å‡ºæ›´ç¡®å®šï¼Œå‡é«˜ï¼ˆe.g., 0.9ï¼‰å¢åŠ å¤šæ ·æ€§ã€‚
   - `max_tokens`ï¼šæ ¹æ®è¾“å‡ºé•¿åº¦è°ƒæ•´ã€‚
3. **æ•°æ®å‡†å¤‡**ï¼šç¡®ä¿`imdb_data.csv`è·¯å¾„æ­£ç¡®ã€‚

å¥½çš„ï¼æˆ‘ä¼šåŸºäºæˆ‘ä»¬çš„éœ€æ±‚ï¼ˆä½¿ç”¨æ¸…æ´—åçš„IMDBæ•°æ®ï¼Œæµ‹è¯•Qwen3-32Bæ¨¡å‹ï¼‰ä¿®æ”¹ä»£ç ï¼Œå¹¶å®ç°ä»¥ä¸‹ä»»åŠ¡ï¼š

1. **ä»»åŠ¡1**ï¼šä½¿ç”¨ä¸‰ç§Promptç­–ç•¥æµ‹è¯•è‡³å°‘200æ¡IMDBæµ‹è¯•é›†æ•°æ®ã€‚
2. **ä»»åŠ¡2**ï¼šè®¾è®¡Promptç”Ÿæˆç»“æ„åŒ–è¾“å‡ºï¼Œæµ‹è¯•è‡³å°‘20æ¡IMDBæµ‹è¯•é›†æ•°æ®ã€‚





### ï¼ä¸‹é¢æ˜¯æˆ‘ä¿®æ”¹åçš„ä»£ç 

```python
import requests
import pandas as pd
import json

# API è®¾ç½®
API_KEY = "your_actual_api_key"  # æ›¿æ¢ä¸ºæˆ‘ä»¬çš„APIå¯†é’¥
URL = 'https://api.gpugeek.com/predictions'
HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json",
    "Stream": "true"
}

# è¯»å–æ¸…æ´—åçš„IMDBæµ‹è¯•é›†æ•°æ®
test_data = pd.read_csv("test_cleaned.csv")  # å‡è®¾æ–‡ä»¶åä¸ºtest_cleaned.csv
reviews = test_data["review"].tolist()  # è·å–è¯„è®ºåˆ—è¡¨

# ä»»åŠ¡1ï¼šæƒ…æ„Ÿåˆ†ç±»ï¼ˆä¸‰ç§Promptç­–ç•¥ï¼Œæµ‹è¯•200æ¡æ•°æ®ï¼‰
def task1_sentiment_classification(reviews, num_samples=200):
    results_strategy1 = []  # ç­–ç•¥1ç»“æœ
    results_strategy2 = []  # ç­–ç•¥2ç»“æœ
    results_strategy3 = []  # ç­–ç•¥3ç»“æœ

    for review in reviews[:num_samples]:  # æµ‹è¯•å‰200æ¡
        # ç­–ç•¥1ï¼šç›´æ¥æƒ…æ„Ÿåˆ¤æ–­
        prompt1 = f"Classify the sentiment of the following movie review as 'positive' or 'negative':\n{review}\nSentiment:"
        data = {
            "model": "GpuGeek/Qwen3-32B",
            "input": {"prompt": prompt1, "max_tokens": 50, "temperature": 0.6, "top_p": 0.7}
        }
        response = requests.post(URL, headers=HEADERS, json=data)
        if response.status_code == 200:
            result = "".join(line.decode("utf-8") for line in response.iter_lines() if line)
            results_strategy1.append(result.strip())
        else:
            print(f"Strategy 1 Error: {response.status_code}, {response.text}")

        # ç­–ç•¥2ï¼šæƒ…æ„Ÿè¯„åˆ†
        prompt2 = f"Rate the sentiment of the following movie review on a scale of 1 to 5, where 1 is very negative and 5 is very positive:\n{review}\nRating:"
        data["input"]["prompt"] = prompt2
        response = requests.post(URL, headers=HEADERS, json=data)
        if response.status_code == 200:
            result = "".join(line.decode("utf-8") for line in response.iter_lines() if line)
            results_strategy2.append(result.strip())
        else:
            print(f"Strategy 2 Error: {response.status_code}, {response.text}")

        # ç­–ç•¥3ï¼šæƒ…æ„Ÿåˆ†æä¸è§£é‡Š
        prompt3 = f"Analyze the sentiment of the following movie review and explain why:\n{review}\nSentiment Analysis:"
        data["input"]["prompt"] = prompt3
        data["input"]["max_tokens"] = 200  # éœ€è¦æ›´å¤štoken
        response = requests.post(URL, headers=HEADERS, json=data)
        if response.status_code == 200:
            result = "".join(line.decode("utf-8") for line in response.iter_lines() if line)
            results_strategy3.append(result.strip())
        else:
            print(f"Strategy 3 Error: {response.status_code}, {response.text}")

    # ä¿å­˜ä»»åŠ¡1ç»“æœ
    with open("results_task1_strategy1.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(results_strategy1))
    with open("results_task1_strategy2.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(results_strategy2))
    with open("results_task1_strategy3.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(results_strategy3))

# ä»»åŠ¡2ï¼šç»“æ„åŒ–è¾“å‡ºï¼ˆæµ‹è¯•20æ¡æ•°æ®ï¼‰
def task2_structured_output(reviews, num_samples=20):
    results_structured = []

    for review in reviews[:num_samples]:  # æµ‹è¯•å‰20æ¡
        prompt = f"Analyze the following movie review, identify its theme and sentiment, and output in JSON format with 'theme' and 'sentiment' fields:\n{review}"
        data = {
            "model": "GpuGeek/Qwen3-32B",
            "input": {"prompt": prompt, "max_tokens": 200, "temperature": 0.6, "top_p": 0.7}
        }
        response = requests.post(URL, headers=HEADERS, json=data)
        if response.status_code == 200:
            result = "".join(line.decode("utf-8") for line in response.iter_lines() if line)
            try:
                json_result = json.loads(result)
                results_structured.append(json_result)
            except json.JSONDecodeError:
                print(f"JSON Parsing Failed: {result}")
        else:
            print(f"Structured Output Error: {response.status_code}, {response.text}")

    # ä¿å­˜ä»»åŠ¡2ç»“æœ
    with open("results_task2.json", "w", encoding="utf-8") as f:
        json.dump(results_structured, f, ensure_ascii=False, indent=2)

# æ‰§è¡Œä»»åŠ¡
task1_sentiment_classification(reviews, num_samples=200)
task2_structured_output(reviews, num_samples=20)

print("ä»»åŠ¡å®Œæˆï¼Œç»“æœå·²ä¿å­˜ï¼")
```

------

### ä»£ç è¯´æ˜

#### 1. **æ•°æ®åŠ è½½**

- ä½¿ç”¨`pandas`è¯»å–æ¸…æ´—åçš„æµ‹è¯•é›†æ•°æ®ï¼ˆ`test_cleaned.csv`ï¼‰ã€‚
- æå–è¯„è®ºåˆ—ï¼ˆ`review`ï¼‰å¹¶è½¬æ¢ä¸ºåˆ—è¡¨ã€‚

#### 2. **ä»»åŠ¡1ï¼šæƒ…æ„Ÿåˆ†ç±»**

- **ç­–ç•¥1**ï¼šç›´æ¥åˆ¤æ–­æƒ…æ„Ÿï¼ˆ`positive`æˆ–`negative`ï¼‰ã€‚
- **ç­–ç•¥2**ï¼šæƒ…æ„Ÿè¯„åˆ†ï¼ˆ1-5ï¼Œ1ä¸ºéå¸¸æ¶ˆæï¼Œ5ä¸ºéå¸¸ç§¯æï¼‰ã€‚
- **ç­–ç•¥3**ï¼šæƒ…æ„Ÿåˆ†æä¸è§£é‡Šã€‚
- å¯¹æ¯æ¡è¯„è®ºè°ƒç”¨APIï¼Œä¿å­˜ç»“æœåˆ°æ–‡ä»¶ã€‚

#### 3. **ä»»åŠ¡2ï¼šç»“æ„åŒ–è¾“å‡º**

- è®¾è®¡Promptè¦æ±‚æ¨¡å‹ç”ŸæˆJSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«`theme`å’Œ`sentiment`å­—æ®µã€‚
- å¯¹20æ¡è¯„è®ºè°ƒç”¨APIï¼Œè§£æå¹¶ä¿å­˜ç»“æœã€‚

#### 4. **ç»“æœä¿å­˜**

- ä»»åŠ¡1ç»“æœä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶ï¼š
  - `results_task1_strategy1.txt`
  - `results_task1_strategy2.txt`
  - `results_task1_strategy3.txt`
- ä»»åŠ¡2ç»“æœä¿å­˜ä¸ºJSONæ–‡ä»¶ï¼š`results_task2.json`ã€‚

### 

#### ä»»åŠ¡1ï¼ˆç­–ç•¥1ï¼‰

```
positive
negative
positive
...
```

#### ä»»åŠ¡1ï¼ˆç­–ç•¥2ï¼‰

```
5
1
4
...
```

#### ä»»åŠ¡1ï¼ˆç­–ç•¥3ï¼‰

```
Sentiment: positive  
Reason: The review uses positive words like "amazing" and "love".
...
```

#### ä»»åŠ¡2

```json
[
  {
    "theme": "movie experience",
    "sentiment": "positive"
  },
  {
    "theme": "plot",
    "sentiment": "negative"
  },
  ...
]
```

------

### æ³¨æ„äº‹é¡¹

1. **APIå¯†é’¥**ï¼šç¡®ä¿æ›¿æ¢`API_KEY`ä¸ºæˆ‘ä»¬çš„å®é™…å¯†é’¥ã€‚
2. **æ•°æ®æ–‡ä»¶**ï¼šç¡®ä¿`test_cleaned.csv`è·¯å¾„æ­£ç¡®ï¼Œä¸”åŒ…å«`review`åˆ—ã€‚
3. **APIé™åˆ¶**ï¼šæ³¨æ„è¯·æ±‚é¢‘ç‡å’Œtokené…é¢ï¼Œé¿å…è¶…é™ã€‚
4. **é”™è¯¯å¤„ç†**ï¼šä»£ç ä¸­å·²åŒ…å«åŸºæœ¬é”™è¯¯å¤„ç†ï¼Œå¦‚APIè°ƒç”¨å¤±è´¥æˆ–JSONè§£æå¤±è´¥ã€‚

------

### æ€»ç»“

é€šè¿‡ä»¥ä¸Šä»£ç ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š

1. ä½¿ç”¨ä¸‰ç§Promptç­–ç•¥æµ‹è¯•200æ¡IMDBæ•°æ®ã€‚
2. ç”Ÿæˆç»“æ„åŒ–è¾“å‡ºå¹¶æµ‹è¯•20æ¡IMDBæ•°æ®ã€‚
3. ä¿å­˜ç»“æœå¹¶è¿›ä¸€æ­¥åˆ†æã€‚

å¦‚æœæˆ‘ä»¬æœ‰å…¶ä»–éœ€æ±‚æˆ–é—®é¢˜ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼

å¥½çš„ï¼ä¸ºäº†è¯„ä¼°Qwen3-32Bæ¨¡å‹åœ¨IMDBæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸Šçš„èƒ½åŠ›ï¼Œæˆ‘ä»¬éœ€è¦è®¾è®¡ä¸€ä¸ªåˆç†çš„è¯„ä¼°æµç¨‹ï¼ŒåŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š

1. **æ•°æ®å‡†å¤‡**ï¼šç¡®ä¿æµ‹è¯•é›†æ•°æ®åŒ…å«çœŸå®æ ‡ç­¾ï¼ˆ`positive`æˆ–`negative`ï¼‰ã€‚
2. **æ¨¡å‹é¢„æµ‹**ï¼šä½¿ç”¨æ¨¡å‹å¯¹æµ‹è¯•é›†æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚
3. **æ€§èƒ½æŒ‡æ ‡è®¡ç®—**ï¼šè®¡ç®—å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰ã€ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰ã€å¬å›ç‡ï¼ˆRecallï¼‰å’ŒF1åˆ†æ•°ï¼ˆF1 Scoreï¼‰ã€‚
4. **ç»“æœåˆ†æ**ï¼šå¯¹æ¯”æ¨¡å‹é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾ï¼Œåˆ†ææ¨¡å‹çš„è¡¨ç°ã€‚

ä»¥ä¸‹æ˜¯å®Œæ•´çš„è¯„ä¼°æµç¨‹å’Œä»£ç å®ç°ã€‚

------

### è¯„ä¼°æµç¨‹

#### 1. æ•°æ®å‡†å¤‡

- ç¡®ä¿`test_cleaned.csv`åŒ…å«ä¸¤åˆ—ï¼š`review`ï¼ˆè¯„è®ºï¼‰å’Œ`sentiment`ï¼ˆçœŸå®æ ‡ç­¾ï¼‰ã€‚
- çœŸå®æ ‡ç­¾åº”ä¸º`positive`æˆ–`negative`ã€‚

#### 2. æ¨¡å‹é¢„æµ‹

- ä½¿ç”¨Qwen3-32Bæ¨¡å‹å¯¹æµ‹è¯•é›†æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚
- ä¿å­˜é¢„æµ‹ç»“æœã€‚

#### 3. æ€§èƒ½æŒ‡æ ‡è®¡ç®—

- ä½¿ç”¨`sklearn`åº“è®¡ç®—ä»¥ä¸‹æŒ‡æ ‡ï¼š
  - **å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰**ï¼šé¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹ã€‚
  - **ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰**ï¼šé¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬ä¸­ï¼Œå®é™…ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ã€‚
  - **å¬å›ç‡ï¼ˆRecallï¼‰**ï¼šå®é™…ä¸ºæ­£ä¾‹çš„æ ·æœ¬ä¸­ï¼Œé¢„æµ‹ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ã€‚
  - **F1åˆ†æ•°ï¼ˆF1 Scoreï¼‰**ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡å€¼ã€‚

#### 4. ç»“æœåˆ†æ

- æ‰“å°æ€§èƒ½æŒ‡æ ‡ã€‚
- åˆ†ææ¨¡å‹åœ¨å“ªäº›æ–¹é¢è¡¨ç°è¾ƒå¥½æˆ–è¾ƒå·®ã€‚

------

### ä»£ç å®ç°

ä»¥ä¸‹æ˜¯å®Œæ•´çš„è¯„ä¼°ä»£ç ï¼ŒåŒ…æ‹¬æ€§èƒ½æŒ‡æ ‡è®¡ç®—å’Œç»“æœåˆ†æã€‚

```python
import requests
import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# API è®¾ç½®
API_KEY = "your_actual_api_key"  # æ›¿æ¢ä¸ºæˆ‘ä»¬çš„APIå¯†é’¥
URL = 'https://api.gpugeek.com/predictions'
HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json",
    "Stream": "true"
}

# è¯»å–æ¸…æ´—åçš„IMDBæµ‹è¯•é›†æ•°æ®
test_data = pd.read_csv("test_cleaned.csv")  # å‡è®¾æ–‡ä»¶åä¸ºtest_cleaned.csv
reviews = test_data["review"].tolist()  # è·å–è¯„è®ºåˆ—è¡¨
true_labels = test_data["sentiment"].tolist()  # è·å–çœŸå®æ ‡ç­¾åˆ—è¡¨

# ä½¿ç”¨Qwen3-32Bæ¨¡å‹è¿›è¡Œé¢„æµ‹
def predict_sentiment(review):
    prompt = f"Classify the sentiment of the following movie review as 'positive' or 'negative':\n{review}\nSentiment:"
    data = {
        "model": "GpuGeek/Qwen3-32B",
        "input": {"prompt": prompt, "max_tokens": 50, "temperature": 0.6, "top_p": 0.7}
    }
    response = requests.post(URL, headers=HEADERS, json=data)
    if response.status_code == 200:
        result = "".join(line.decode("utf-8") for line in response.iter_lines() if line)
        return result.strip().lower()  # è¿”å›å°å†™ç»“æœ
    else:
        print(f"Error: {response.status_code}, {response.text}")
        return None

# å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
predicted_labels = []
for review in reviews:
    predicted_label = predict_sentiment(review)
    if predicted_label:
        predicted_labels.append(predicted_label)
    else:
        predicted_labels.append("unknown")  # å¤„ç†é¢„æµ‹å¤±è´¥çš„æƒ…å†µ

# ç¡®ä¿é¢„æµ‹ç»“æœå’ŒçœŸå®æ ‡ç­¾é•¿åº¦ä¸€è‡´
if len(predicted_labels) != len(true_labels):
    print("Warning: Predicted labels and true labels have different lengths!")

# è®¡ç®—æ€§èƒ½æŒ‡æ ‡
def evaluate_performance(true_labels, predicted_labels):
    # è¿‡æ»¤æ‰æœªçŸ¥ç»“æœ
    filtered_true_labels = []
    filtered_predicted_labels = []
    for true, pred in zip(true_labels, predicted_labels):
        if pred != "unknown":
            filtered_true_labels.append(true)
            filtered_predicted_labels.append(pred)

    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(filtered_true_labels, filtered_predicted_labels)
    precision = precision_score(filtered_true_labels, filtered_predicted_labels, pos_label="positive")
    recall = recall_score(filtered_true_labels, filtered_predicted_labels, pos_label="positive")
    f1 = f1_score(filtered_true_labels, filtered_predicted_labels, pos_label="positive")

    return accuracy, precision, recall, f1

# è¯„ä¼°æ¨¡å‹æ€§èƒ½
accuracy, precision, recall, f1 = evaluate_performance(true_labels, predicted_labels)

# æ‰“å°ç»“æœ
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

# ä¿å­˜é¢„æµ‹ç»“æœ
results_df = pd.DataFrame({
    "review": reviews,
    "true_sentiment": true_labels,
    "predicted_sentiment": predicted_labels
})
results_df.to_csv("model_predictions.csv", index=False)

print("è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜ï¼")
```

------

### ä»£ç è¯´æ˜

#### 1. **æ•°æ®å‡†å¤‡**

- è¯»å–`test_cleaned.csv`ï¼Œæå–`review`å’Œ`sentiment`åˆ—ã€‚

#### 2. **æ¨¡å‹é¢„æµ‹**

- ä½¿ç”¨Qwen3-32Bæ¨¡å‹å¯¹æ¯æ¡è¯„è®ºè¿›è¡Œé¢„æµ‹ã€‚
- å°†é¢„æµ‹ç»“æœè½¬æ¢ä¸ºå°å†™ï¼ˆ`positive`æˆ–`negative`ï¼‰ã€‚
- å¤„ç†é¢„æµ‹å¤±è´¥çš„æƒ…å†µï¼ˆæ ‡è®°ä¸º`unknown`ï¼‰ã€‚

#### 3. **æ€§èƒ½æŒ‡æ ‡è®¡ç®—**

- è¿‡æ»¤æ‰`unknown`ç»“æœã€‚
- ä½¿ç”¨`sklearn`è®¡ç®—å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°ã€‚

#### 4. **ç»“æœä¿å­˜**

- å°†é¢„æµ‹ç»“æœä¿å­˜ä¸º`model_predictions.csv`ï¼ŒåŒ…å«è¯„è®ºã€çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾ã€‚

------

### ç¤ºä¾‹è¾“å‡º

#### æ§åˆ¶å°è¾“å‡º

```
Accuracy: 0.8765
Precision: 0.8902
Recall: 0.8621
F1 Score: 0.8759
```

#### `model_predictions.csv`

| review                   | true_sentiment | predicted_sentiment |
| ------------------------ | -------------- | ------------------- |
| "This movie is amazing!" | positive       | positive            |
| "The plot was boring."   | negative       | negative            |
| "I loved the acting."    | positive       | positive            |
| ...                      | ...            | ...                 |

------

### æ€§èƒ½æŒ‡æ ‡è§£é‡Š

1. **å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰**ï¼šæ¨¡å‹é¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹ã€‚ä¾‹å¦‚ï¼Œ0.8765è¡¨ç¤º87.65%çš„é¢„æµ‹æ˜¯æ­£ç¡®çš„ã€‚
2. **ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰**ï¼šæ¨¡å‹é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ ·æœ¬ä¸­ï¼Œå®é™…ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ã€‚ä¾‹å¦‚ï¼Œ0.8902è¡¨ç¤º89.02%çš„â€œpositiveâ€é¢„æµ‹æ˜¯æ­£ç¡®çš„ã€‚
3. **å¬å›ç‡ï¼ˆRecallï¼‰**ï¼šå®é™…ä¸ºæ­£ä¾‹çš„æ ·æœ¬ä¸­ï¼Œæ¨¡å‹é¢„æµ‹ä¸ºæ­£ä¾‹çš„æ¯”ä¾‹ã€‚ä¾‹å¦‚ï¼Œ0.8621è¡¨ç¤º86.21%çš„æ­£ä¾‹è¢«æ­£ç¡®é¢„æµ‹ã€‚
4. **F1åˆ†æ•°ï¼ˆF1 Scoreï¼‰**ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡å€¼ï¼Œç»¼åˆè¡¡é‡æ¨¡å‹æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œ0.8759è¡¨ç¤ºæ¨¡å‹åœ¨æ­£ä¾‹åˆ†ç±»ä¸Šè¡¨ç°è‰¯å¥½ã€‚





# ä¸ºä»€ä¹ˆè¿™é‡ŒEmbeddingæ•ˆæœä¸ä½³

åœ¨æˆ‘ä»¬çš„ IMDB æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨ **è¯é¢‘é€‰å–ç‰¹å¾** çš„æ–¹æ³•åœ¨ç®€å•æ¨¡å‹ï¼ˆå¦‚ Logistic å›å½’ã€é«˜æ–¯æœ´ç´ è´å¶æ–¯ï¼‰å’Œå¾®è°ƒçš„ BERT æ¨¡å‹ä¸Šè¡¨ç°ä¼˜äº **DistilBERT è¯åµŒå…¥** æ–¹æ³•ï¼Œè¿™å¯èƒ½æ˜¯ç”±ä»¥ä¸‹å‡ ä¸ªåŸå› é€ æˆçš„ï¼š

------

### 1. **æ•°æ®ç‰¹æ€§ä¸ä»»åŠ¡åŒ¹é…**

- **IMDB æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡** æ˜¯ä¸€ä¸ªç›¸å¯¹ç®€å•çš„äºŒåˆ†ç±»ä»»åŠ¡ï¼Œæƒ…æ„Ÿææ€§ï¼ˆæ­£é¢/è´Ÿé¢ï¼‰é€šå¸¸å¯ä»¥é€šè¿‡å…³é”®è¯ï¼ˆå¦‚â€œgreatâ€ã€â€œterribleâ€ï¼‰ç›´æ¥åˆ¤æ–­ã€‚
- **è¯é¢‘é€‰å–ç‰¹å¾** æ–¹æ³•èƒ½å¤Ÿç›´æ¥æ•æ‰è¿™äº›å…³é”®è¯çš„å‡ºç°é¢‘ç‡ï¼Œè€Œ **DistilBERT è¯åµŒå…¥** åˆ™æ›´æ³¨é‡ä¸Šä¸‹æ–‡è¯­ä¹‰ï¼Œå¯èƒ½å¯¹ç®€å•ä»»åŠ¡â€œè¿‡åº¦æ‹Ÿåˆâ€æˆ–â€œè¿‡åº¦å¤æ‚åŒ–â€ã€‚
- ç ”ç©¶æ˜¾ç¤ºï¼Œå¯¹äºç®€å•çš„åˆ†ç±»ä»»åŠ¡ï¼Œä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ TF-IDF + Logistic å›å½’ï¼‰å¾€å¾€è¡¨ç°ä¼˜å¼‚ï¼Œå› ä¸ºå®ƒä»¬æ›´ç›´æ¥åœ°åˆ©ç”¨äº†ä»»åŠ¡çš„å…³é”®ç‰¹å¾ [1](https://arxiv.org/abs/1905.05583).

------

### 2. **è¯åµŒå…¥çš„å…¨å±€æ± åŒ–é—®é¢˜**

- æˆ‘ä»¬æåˆ°ä½¿ç”¨äº† **DistilBERT è¯åµŒå…¥** å¹¶ä¿å­˜ä¸º `.npy` æ–‡ä»¶ï¼Œè¿™å¯èƒ½æ˜¯é€šè¿‡å…¨å±€å¹³å‡æ± åŒ–ï¼ˆæˆ–å…¶ä»–æ± åŒ–æ–¹æ³•ï¼‰å°†åºåˆ—åµŒå…¥å‹ç¼©ä¸ºå•ä¸ªå‘é‡ã€‚
- è¿™ç§æ± åŒ–æ“ä½œå¯èƒ½ä¼šä¸¢å¤±åºåˆ—ä¸­çš„é‡è¦å±€éƒ¨ä¿¡æ¯ï¼ˆå¦‚å…³é”®è¯çš„ä½ç½®å’Œä¸Šä¸‹æ–‡ï¼‰ï¼Œä»è€Œé™ä½æ¨¡å‹æ€§èƒ½ã€‚
- ç ”ç©¶è¡¨æ˜ï¼ŒBERT ç­‰æ¨¡å‹åœ¨å¤„ç†å…¨å±€æ± åŒ–åçš„åµŒå…¥æ—¶ï¼Œæ€§èƒ½å¯èƒ½ä¸å¦‚åŸå§‹åºåˆ—åµŒå…¥ [2](https://arxiv.org/abs/1908.08962).

------

### 3. **å™ªå£°ä¸æ•°æ®é¢„å¤„ç†**

- è™½ç„¶æˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œäº†æ¸…æ´—ï¼ˆå¦‚å»é™¤åœç”¨è¯ã€è¿˜åŸè¯æ ¹ï¼‰ï¼Œä½† IMDB æ•°æ®é›†ä¸­çš„æ–‡æœ¬å¯èƒ½ä»ç„¶åŒ…å«å™ªå£°ï¼ˆå¦‚æ‹¼å†™é”™è¯¯ã€ä¿šè¯­ã€éæ ‡å‡†è¡¨è¾¾ï¼‰ã€‚
- **è¯é¢‘é€‰å–ç‰¹å¾** æ–¹æ³•å¯¹å™ªå£°çš„é²æ£’æ€§è¾ƒå¼ºï¼Œå› ä¸ºå™ªå£°è¯æ±‡çš„å‡ºç°é¢‘ç‡é€šå¸¸è¾ƒä½ï¼Œå¯¹æ•´ä½“ç‰¹å¾å½±å“è¾ƒå°ã€‚
- **DistilBERT è¯åµŒå…¥** å¯¹å™ªå£°æ›´æ•æ„Ÿï¼Œå› ä¸ºé¢„è®­ç»ƒæ¨¡å‹å¯èƒ½ä¼šé”™è¯¯åœ°æ•æ‰å™ªå£°çš„è¯­ä¹‰ï¼Œä»è€Œå½±å“åˆ†ç±»æ€§èƒ½ [3](https://arxiv.org/abs/2004.03705).

------

### 4. **ç‰¹å¾ç»´åº¦ä¸æ¨¡å‹å¤æ‚åº¦**

- **è¯é¢‘é€‰å–ç‰¹å¾** çš„ç»´åº¦é€šå¸¸è¾ƒä½ï¼ˆå¦‚å‡ åƒç»´ï¼‰ï¼Œé€‚åˆç®€å•æ¨¡å‹ï¼ˆå¦‚ Logistic å›å½’ã€æœ´ç´ è´å¶æ–¯ï¼‰å¿«é€Ÿè®­ç»ƒå’Œæ¨ç†ã€‚
- **DistilBERT è¯åµŒå…¥** çš„ç»´åº¦è¾ƒé«˜ï¼ˆå¦‚ 768 ç»´ï¼‰ï¼Œå¯èƒ½éœ€è¦æ›´å¤æ‚çš„æ¨¡å‹ï¼ˆå¦‚ç¥ç»ç½‘ç»œï¼‰æ¥å……åˆ†æ•æ‰ç‰¹å¾ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆã€‚
- ç ”ç©¶æ˜¾ç¤ºï¼Œé«˜ç»´åµŒå…¥åœ¨ç®€å•æ¨¡å‹ä¸Šè¡¨ç°ä¸ä½³ï¼Œå› ä¸ºæ¨¡å‹æ— æ³•æœ‰æ•ˆå¤„ç†é«˜ç»´ç¨€ç–ç‰¹å¾ [4](https://arxiv.org/abs/1907.11692).

------

### 5. **å¾®è°ƒç­–ç•¥ä¸æ¨¡å‹é€‚é…**

- å¦‚æœç›´æ¥ä½¿ç”¨ **DistilBERT è¯åµŒå…¥** è€Œä¸è¿›è¡Œå¾®è°ƒï¼Œæ¨¡å‹å¯èƒ½æ— æ³•å……åˆ†é€‚åº” IMDB ä»»åŠ¡ã€‚
- å¾®è°ƒéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºå’Œè¶…å‚æ•°è°ƒæ•´ï¼Œå¦‚æœå¾®è°ƒä¸å……åˆ†ï¼Œæ¨¡å‹æ€§èƒ½å¯èƒ½ä¸å¦‚ä¼ ç»Ÿæ–¹æ³•ã€‚
- ç ”ç©¶è¡¨æ˜ï¼ŒBERT ç­‰æ¨¡å‹åœ¨å¾®è°ƒä¸è¶³æ—¶ï¼Œæ€§èƒ½å¯èƒ½ä¸å¦‚ä¼ ç»Ÿæ–¹æ³• [5](https://arxiv.org/abs/1905.05583).

------

### 6. **ä»»åŠ¡ç‰¹æ€§ä¸æ¨¡å‹åŒ¹é…**

- **IMDB æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡** çš„æ–‡æœ¬é•¿åº¦é€šå¸¸è¾ƒçŸ­ï¼ˆå‡ ååˆ°å‡ ç™¾ä¸ªå•è¯ï¼‰ï¼Œæƒ…æ„Ÿææ€§å¯ä»¥é€šè¿‡å±€éƒ¨å…³é”®è¯åˆ¤æ–­ã€‚
- **è¯é¢‘é€‰å–ç‰¹å¾** æ–¹æ³•èƒ½å¤Ÿç›´æ¥æ•æ‰è¿™äº›å…³é”®è¯ï¼Œè€Œ **DistilBERT è¯åµŒå…¥** åˆ™æ›´æ³¨é‡å…¨å±€è¯­ä¹‰ï¼Œå¯èƒ½å¯¹ç®€å•ä»»åŠ¡â€œè¿‡åº¦å¤æ‚åŒ–â€ã€‚
- ç ”ç©¶æ˜¾ç¤ºï¼Œå¯¹äºçŸ­æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œä¼ ç»Ÿæ–¹æ³•å¾€å¾€è¡¨ç°ä¼˜å¼‚ï¼Œå› ä¸ºå®ƒä»¬æ›´ç›´æ¥åœ°åˆ©ç”¨äº†ä»»åŠ¡çš„å…³é”®ç‰¹å¾ [6](https://arxiv.org/abs/1907.11692).

------

### æ”¹è¿›å»ºè®®

1. **é‡æ–°ç”Ÿæˆè¯åµŒå…¥**ï¼š
   - ä½¿ç”¨ **DistilBERT** ç”Ÿæˆå®Œæ•´çš„åºåˆ—åµŒå…¥ï¼ˆ`batch_size, seq_length, hidden_size`ï¼‰ï¼Œè€Œä¸æ˜¯å…¨å±€æ± åŒ–åçš„åµŒå…¥ã€‚
   - å°†å®Œæ•´åºåˆ—åµŒå…¥è¾“å…¥åˆ°å¾®è°ƒçš„ BERT æ¨¡å‹ä¸­ï¼Œä»¥æ›´å¥½åœ°æ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
2. **ä¼˜åŒ–å¾®è°ƒç­–ç•¥**ï¼š
   - åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œè°ƒæ•´å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°ç­‰è¶…å‚æ•°ï¼Œé¿å…è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆã€‚
   - ä½¿ç”¨æ›´é•¿çš„è®­ç»ƒæ—¶é—´æˆ–æ›´å¤§çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚
3. **ç»“åˆä¼ ç»Ÿæ–¹æ³•ä¸é¢„è®­ç»ƒæ¨¡å‹**ï¼š
   - å°† **è¯é¢‘é€‰å–ç‰¹å¾** ä¸ **DistilBERT è¯åµŒå…¥** ç»“åˆï¼Œåˆ©ç”¨äºŒè€…çš„ä¼˜åŠ¿ï¼Œæé«˜æ¨¡å‹æ€§èƒ½ã€‚
4. **æ¸…æ´—æ•°æ®**ï¼š
   - è¿›ä¸€æ­¥åŠ å¼ºæ•°æ®é¢„å¤„ç†ï¼Œå»é™¤å™ªå£°ï¼Œè§„èŒƒæ–‡æœ¬æ ¼å¼ï¼Œä»¥æé«˜æ¨¡å‹çš„è¾“å…¥è´¨é‡ã€‚



#### DistilBERT è¯åµŒå…¥çš„å¾®è°ƒ

------

## 1. **å¦‚ä½•è¿›è¡Œ DistilBERT è¯åµŒå…¥çš„å¾®è°ƒï¼Ÿ**

DistilBERT æ˜¯ BERT çš„è½»é‡çº§ç‰ˆæœ¬ï¼Œé€‚åˆéœ€è¦å¿«é€Ÿæ¨ç†çš„åœºæ™¯ã€‚å¾®è°ƒ DistilBERT çš„æµç¨‹å¦‚ä¸‹ï¼š

### ç¯å¢ƒè®¾ç½®

é¦–å…ˆï¼Œå®‰è£…æ‰€éœ€çš„åº“ï¼š

```bash
pip install torch transformers datasets
```

### æ•°æ®å‡†å¤‡

åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›†ï¼š

```python
from datasets import load_dataset
from transformers import DistilBertTokenizerFast

# åŠ è½½æ•°æ®é›†ï¼ˆä»¥ IMDB ä¸ºä¾‹ï¼‰
dataset = load_dataset("imdb")

# åŠ è½½ DistilBERT tokenizer
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

# æ•°æ®é¢„å¤„ç†å‡½æ•°
def preprocess_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# å¯¹æ•°æ®é›†è¿›è¡Œåˆ†è¯
tokenized_datasets = dataset.map(preprocess_function, batched=True)
```

### æ¨¡å‹åŠ è½½

åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œå¾®è°ƒï¼š

```python
from transformers import DistilBertForSequenceClassification
from transformers import Trainer, TrainingArguments

# åŠ è½½ DistilBERT æ¨¡å‹
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

# è®¾ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)

# åˆå§‹åŒ– Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

### è¯„ä¼°æ¨¡å‹

è®­ç»ƒå®Œæˆåï¼Œè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼š

```python
results = trainer.evaluate()
print(results)
```





**IMDB æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡**æ˜¯ä¸€ä¸ªå…¸å‹çš„è‹±æ–‡çŸ­æ–‡æœ¬åˆ†ç±»ä»»åŠ¡

#### 1. **ä½¿ç”¨ DistilBERT æˆ– RoBERTa è¿›è¡Œå¾®è°ƒ**

- DistilBERT æ˜¯ BERT çš„è½»é‡çº§ç‰ˆæœ¬ï¼Œé€‚åˆå¿«é€Ÿæ¨ç†å’Œèµ„æºæœ‰é™çš„ç¯å¢ƒã€‚
- RoBERTa æ˜¯ BERT çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œåœ¨ IMDB ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

**ä»£ç ç¤ºä¾‹**ï¼š

```python
from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset

# åŠ è½½æ•°æ®é›†
dataset = load_dataset("imdb")

# åŠ è½½æ¨¡å‹
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

# è®¾ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)

# åˆå§‹åŒ– Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

#### 2. **ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚ TF-IDF + Logistic å›å½’ï¼‰**

- å¦‚æœèµ„æºæœ‰é™æˆ–ä»»åŠ¡ç®€å•ï¼Œä¼ ç»Ÿæ–¹æ³•å¯èƒ½æ›´é«˜æ•ˆã€‚
- ç ”ç©¶è¡¨æ˜ï¼Œå¯¹äºçŸ­æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œä¼ ç»Ÿæ–¹æ³•å¾€å¾€è¡¨ç°ä¼˜å¼‚ã€‚

**ä»£ç ç¤ºä¾‹**ï¼š

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# åŠ è½½æ•°æ®é›†
train_texts = ["sample text 1", "sample text 2"]
train_labels = [0, 1]

# æå– TF-IDF ç‰¹å¾
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(train_texts)

# è®­ç»ƒ Logistic å›å½’æ¨¡å‹
model = LogisticRegression()
model.fit(X_train, train_labels)

# é¢„æµ‹
test_texts = ["sample text 3"]
X_test = vectorizer.transform(test_texts)
preds = model.predict(X_test)
print(preds)
```

#### 3. **ä½¿ç”¨è½»é‡çº§åµŒå…¥æ¨¡å‹ï¼ˆå¦‚ `jina-embeddings-v2-base-en`ï¼‰**

- å¦‚æœéœ€è¦ä½¿ç”¨åµŒå…¥æ¨¡å‹ï¼Œå¯ä»¥é€‰æ‹©æ›´è½»é‡çº§çš„ç‰ˆæœ¬ï¼ˆå¦‚ `jina-embeddings-v2-base-en`ï¼‰ï¼Œå®ƒä¸“é—¨é’ˆå¯¹è‹±æ–‡ä»»åŠ¡è®¾è®¡ã€‚

**ä»£ç ç¤ºä¾‹**ï¼š

```python
from transformers import AutoTokenizer, AutoModel
import torch

# åŠ è½½æ¨¡å‹å’Œ tokenizer
model_name = "jinaai/jina-embeddings-v2-base-en"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# ç”ŸæˆåµŒå…¥
text = "This is a sample sentence."
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
with torch.no_grad():
    outputs = model(**inputs)

# è·å–åµŒå…¥å‘é‡
embeddings = outputs.last_hidden_state.mean(dim=1)
print(embeddings)
```





æœ€ç»ˆè®¡åˆ’ **å¾®è°ƒ DistilBERT** æ¥å®Œæˆ IMDB æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ï¼Œé‚£ä¹ˆ **åœ¨å¾®è°ƒä¹‹å‰ä½¿ç”¨ DistilBERT è¿›è¡Œè¯åµŒå…¥ï¼ˆç‰¹å¾æå–ï¼‰æ˜¯å¤šä½™çš„**ã€‚

### ä¸ºä»€ä¹ˆä¸éœ€è¦é¢„å…ˆæå–è¯åµŒå…¥ï¼Ÿ

1. **å¾®è°ƒè¿‡ç¨‹ä¼šä¼˜åŒ–æ¨¡å‹**ï¼š
   - å¾®è°ƒ DistilBERT æ—¶ï¼Œæ¨¡å‹çš„ **æ‰€æœ‰å‚æ•°**ï¼ˆåŒ…æ‹¬åµŒå…¥å±‚ï¼‰éƒ½ä¼šæ ¹æ®ä»»åŠ¡æ•°æ®è¿›è¡Œè°ƒæ•´ã€‚
   - è¿™æ„å‘³ç€å¾®è°ƒåçš„æ¨¡å‹ä¼šç›´æ¥å­¦ä¹ åˆ°é€‚åˆä»»åŠ¡çš„ç‰¹å¾ï¼Œä¸éœ€è¦é¢„å…ˆæå–åµŒå…¥ã€‚
2. **ç«¯åˆ°ç«¯è®­ç»ƒçš„ä¼˜åŠ¿**ï¼š
   - å¾®è°ƒæ˜¯ä¸€ç§ **ç«¯åˆ°ç«¯** çš„è®­ç»ƒæ–¹å¼ï¼Œç›´æ¥ä»åŸå§‹æ–‡æœ¬è¾“å…¥åˆ°åˆ†ç±»è¾“å‡ºï¼Œé¿å…äº†é¢å¤–çš„ç‰¹å¾æå–æ­¥éª¤ã€‚
   - è¿™ç§æ–¹æ³•é€šå¸¸æ¯”â€œå…ˆæå–åµŒå…¥ï¼Œå†è®­ç»ƒåˆ†ç±»å™¨â€æ›´é«˜æ•ˆä¸”æ€§èƒ½æ›´å¥½ã€‚
3. **é¢„æå–åµŒå…¥çš„å±€é™æ€§**ï¼š
   - é¢„å…ˆæå–çš„åµŒå…¥æ˜¯ **é™æ€çš„**ï¼Œæ— æ³•æ ¹æ®ä»»åŠ¡æ•°æ®è¿›è¡Œä¼˜åŒ–ã€‚
   - è¿™ç§é™æ€åµŒå…¥å¯èƒ½æ— æ³•å……åˆ†æ•æ‰ä»»åŠ¡ç›¸å…³çš„ç‰¹å¾ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

------

### **å¾®è°ƒ DistilBERT çš„æµç¨‹**

ä»¥ä¸‹æ˜¯ç›´æ¥å¾®è°ƒ DistilBERT çš„å®Œæ•´æµç¨‹ï¼š

#### 1. **åŠ è½½æ•°æ®é›†**

```python
from datasets import load_dataset

# åŠ è½½ IMDB æ•°æ®é›†
dataset = load_dataset("imdb")
```

#### 2. **åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨**

```python
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification

# åŠ è½½ DistilBERT åˆ†è¯å™¨
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

# åŠ è½½ DistilBERT æ¨¡å‹
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)
```

#### 3. **æ•°æ®é¢„å¤„ç†**

```python
# å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
def preprocess_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# åº”ç”¨åˆ†è¯å™¨
tokenized_datasets = dataset.map(preprocess_function, batched=True)
```

#### 4. **è®¾ç½®è®­ç»ƒå‚æ•°**

```python
from transformers import TrainingArguments

# è®¾ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)
```

#### 5. **åˆå§‹åŒ– Trainer**

```python
from transformers import Trainer

# åˆå§‹åŒ– Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
)
```

#### 6. **å¼€å§‹è®­ç»ƒ**

```python
# å¾®è°ƒæ¨¡å‹
trainer.train()
```

#### 7. **è¯„ä¼°æ¨¡å‹**

```python
# è¯„ä¼°æ¨¡å‹æ€§èƒ½
results = trainer.evaluate()
print(results)
```

------

### **ä¸ºä»€ä¹ˆå¾®è°ƒä¼˜äºé¢„æå–åµŒå…¥ï¼Ÿ**

1. **ä»»åŠ¡é€‚åº”æ€§**ï¼š
   - å¾®è°ƒåçš„æ¨¡å‹ä¼šç›´æ¥å­¦ä¹ åˆ°é€‚åˆä»»åŠ¡çš„ç‰¹å¾ï¼Œè€Œé¢„æå–çš„åµŒå…¥æ˜¯é€šç”¨çš„ï¼Œå¯èƒ½ä¸é€‚åˆç‰¹å®šä»»åŠ¡ã€‚
2. **æ€§èƒ½æå‡**ï¼š
   - ç ”ç©¶è¡¨æ˜ï¼Œå¾®è°ƒæ¨¡å‹åœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜äºé¢„æå–åµŒå…¥çš„æ–¹æ³• [1](https://medium.com/data-science/feature-extraction-with-bert-for-text-classification-533dde44dc2f).
3. **ç®€åŒ–æµç¨‹**ï¼š
   - å¾®è°ƒæ˜¯ç«¯åˆ°ç«¯çš„ï¼Œä¸éœ€è¦é¢å¤–çš„ç‰¹å¾æå–æ­¥éª¤ï¼Œç®€åŒ–äº†æµç¨‹å¹¶å‡å°‘äº†è®¡ç®—æˆæœ¬ã€‚





